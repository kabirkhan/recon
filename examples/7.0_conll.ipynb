{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "168f7a83b8d2426085f1862cb369f7db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/2.58k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b5e188aa7094e89823a24287963eec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading metadata:   0%|          | 0.00/1.62k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2003/conll2003 (download: 959.94 KiB, generated: 9.78 MiB, post-processed: Unknown size, total: 10.72 MiB) to /Users/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4a844026c9a4e55a997234ce280af15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/983k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e25a7b19fd6b4d3ea9a7bf8a8dab4aad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/14042 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "185412f35c8e409baef135247ed076fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/3251 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d7198812824cbd882bbe7448e4aa90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/3454 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /Users/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "947d8bd22d1a45f297bdce23cabd72e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conll2003 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span as SpacySpan\n",
    "from spacy.training.iob_utils import offsets_from_biluo_tags, spans_from_biluo_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_entities(tags):\n",
    "    entities = []\n",
    "    prev_tag = \"O\"\n",
    "    start = None\n",
    "    for i, tag in enumerate(tags):\n",
    "        print(tag)\n",
    "        if tag is None:\n",
    "            continue\n",
    "        if tag.startswith(\"O\"):\n",
    "            # TODO: We shouldn't be getting these malformed inputs. Fix this.\n",
    "            if start is not None:\n",
    "                start = None\n",
    "            continue\n",
    "#         elif tag == \"-\":\n",
    "#             continue\n",
    "#         elif tag.startswith(\"I\"):\n",
    "#             if start is None:\n",
    "#                 raise ValueError(Errors.E067.format(tags=tags[:i + 1]))\n",
    "#             continue\n",
    "        if tag.startswith(\"B\"):\n",
    "            start = i\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag:\n",
    "            continue\n",
    "        elif prev_tag != \"O\" and tag == \"O\":\n",
    "            entities.append((tag[2:], start, i))\n",
    "            start = None\n",
    "#         elif tag.startswith(\"L\"):\n",
    "#             entities.append((tag[2:], start, i))\n",
    "#             start = None\n",
    "        else:\n",
    "            raise ValueError(Errors.E068.format(tag=tag))\n",
    "        prev_tag = tag\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_entities(tags):\n",
    "    entities = []\n",
    "    start = None\n",
    "    prev_tag = \"O\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag is None:\n",
    "            continue\n",
    "        if tag.startswith(\"I\"):\n",
    "            if start is None:\n",
    "                if i == 0:\n",
    "                    start = 0\n",
    "                else:\n",
    "                    raise\n",
    "            continue\n",
    "        elif tag.startswith(\"B\"):\n",
    "            start = i\n",
    "        elif (prev_tag.startswith(\"I\") or prev_tag.startswith(\"B\")) and tag != \"I\":\n",
    "            entities.append((prev_tag[2:], start, i - 1))\n",
    "            start = None\n",
    "        else:\n",
    "            start = None\n",
    "        prev_tag = tag\n",
    "        \n",
    "    if start:\n",
    "        entities.append((tag[2:], start, i))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_from_bio_tags(doc, tags):\n",
    "    \"\"\"Encode per-token tags following the BILUO scheme into Span object, e.g.\n",
    "    to overwrite the doc.ents.\n",
    "    doc (Doc): The document that the BILUO tags refer to.\n",
    "    entities (iterable): A sequence of BILUO tags with each tag describing one\n",
    "        token. Each tags string will be of the form of either \"\", \"O\" or\n",
    "        \"{action}-{label}\", where action is one of \"B\", \"I\", \"L\", \"U\".\n",
    "    RETURNS (list): A sequence of Span objects.\n",
    "    \"\"\"\n",
    "    token_offsets = tags_to_entities(tags)\n",
    "    spans = []\n",
    "    for label, start_idx, end_idx in token_offsets:\n",
    "        span = SpacySpan(doc, start_idx, end_idx + 1, label=label)\n",
    "        spans.append(span)\n",
    "    return spans\n",
    "\n",
    "def offsets_from_bio_tags(doc, tags):\n",
    "    \"\"\"Encode per-token tags following the BILUO scheme into entity offsets.\n",
    "    doc (Doc): The document that the BILUO tags refer to.\n",
    "    entities (iterable): A sequence of BILUO tags with each tag describing one\n",
    "        token. Each tags string will be of the form of either \"\", \"O\" or\n",
    "        \"{action}-{label}\", where action is one of \"B\", \"I\", \"L\", \"U\".\n",
    "    RETURNS (list): A sequence of `(start, end, label)` triples. `start` and\n",
    "        `end` will be character-offset integers denoting the slice into the\n",
    "        original string.\n",
    "    \"\"\"\n",
    "    spans = spans_from_bio_tags(doc, tags)\n",
    "    return [(span.start_char, span.end_char, span.label_) for span in spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.types import Example, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recon_examples(dataset, labels=None, labels_property = \"ner_tags\"):\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    for i, e in enumerate(dataset):\n",
    "        doc = Doc(nlp.vocab, words=e[\"tokens\"], spaces=[True] * len(e[\"tokens\"]))\n",
    "        \n",
    "        if labels:\n",
    "            tags = [labels[tag_n] for tag_n in e[labels_property]]\n",
    "        else:\n",
    "            tags = e[labels_property]\n",
    "        try:\n",
    "            offsets = offsets_from_bio_tags(doc, tags)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"ERROR AT INDEX {i}\")\n",
    "            print(tags)\n",
    "\n",
    "\n",
    "        spans = [Span(text=doc.text[e[0]:e[1]], start=e[0], end=e[1], label=e[2]) for e in offsets]\n",
    "        examples.append(Example(text=doc.text, spans=spans))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = make_recon_examples(conll2003[\"train\"], conll_labels)\n",
    "dev = make_recon_examples(conll2003[\"validation\"], conll_labels)\n",
    "test = make_recon_examples(conll2003[\"test\"], conll_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.corpus import Corpus\n",
    "from recon.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003_corpus = Corpus(Dataset(\"train\", train), Dataset(\"dev\", dev), Dataset(\"test\", test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon import get_ner_stats, get_entity_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='u.s.', label='LOC', count=154, examples=[]),\n",
       " EntityCoverage(text='germany', label='LOC', count=97, examples=[]),\n",
       " EntityCoverage(text='london', label='LOC', count=82, examples=[]),\n",
       " EntityCoverage(text='australia', label='LOC', count=80, examples=[]),\n",
       " EntityCoverage(text='france', label='LOC', count=80, examples=[]),\n",
       " EntityCoverage(text='russia', label='LOC', count=79, examples=[]),\n",
       " EntityCoverage(text='world cup', label='MISC', count=77, examples=[]),\n",
       " EntityCoverage(text='italy', label='LOC', count=64, examples=[]),\n",
       " EntityCoverage(text='china', label='LOC', count=58, examples=[]),\n",
       " EntityCoverage(text='england', label='LOC', count=57, examples=[])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = get_entity_coverage(conll2003_corpus.all)\n",
    "\n",
    "ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='clinton', label='PER', count=23, examples=[]),\n",
       " EntityCoverage(text='yeltsin', label='PER', count=19, examples=[]),\n",
       " EntityCoverage(text='wang', label='PER', count=19, examples=[]),\n",
       " EntityCoverage(text='lebed', label='PER', count=17, examples=[]),\n",
       " EntityCoverage(text='arafat', label='PER', count=14, examples=[]),\n",
       " EntityCoverage(text='suu kyi', label='PER', count=14, examples=[]),\n",
       " EntityCoverage(text='edberg', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='albright', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='lara', label='PER', count=12, examples=[]),\n",
       " EntityCoverage(text='dole', label='PER', count=11, examples=[])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_ec = [e for e in ec if e.label == \"PER\"]\n",
    "\n",
    "per_ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2223"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(per_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{\n",
      "    \"n_examples\":3251,\n",
      "    \"n_examples_no_entities\":702,\n",
      "    \"n_annotations\":5842,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":1826,\n",
      "        \"PER\":1816,\n",
      "        \"ORG\":1332,\n",
      "        \"MISC\":868\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "dev\n",
      "{\n",
      "    \"n_examples\":3454,\n",
      "    \"n_examples_no_entities\":738,\n",
      "    \"n_annotations\":5553,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":1651,\n",
      "        \"ORG\":1644,\n",
      "        \"PER\":1591,\n",
      "        \"MISC\":667\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "test\n",
      "{\n",
      "    \"n_examples\":0,\n",
      "    \"n_examples_no_entities\":0,\n",
      "    \"n_annotations\":0,\n",
      "    \"n_annotations_per_type\":{\n",
      "\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "all\n",
      "{\n",
      "    \"n_examples\":6705,\n",
      "    \"n_examples_no_entities\":1440,\n",
      "    \"n_annotations\":11395,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":3477,\n",
      "        \"PER\":3407,\n",
      "        \"ORG\":2976,\n",
      "        \"MISC\":1535\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for name, stats in conll2003_corpus.apply(get_ner_stats, serialize=True).items():\n",
    "    print(name)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.insights import get_label_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_disparities(conll2003_corpus.test, \"PER\", \"LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
