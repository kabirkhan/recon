{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding Errors in the Conll 2003 NER Dataset\n",
    "\n",
    "Would it surprise you to know that one of the most used and widely known benchmark NER datasets has errors in it? Well there definitely are :) \n",
    "In this notebook we're going to walk through finding and correcting some of these errors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "To get the Conll NER data we'll use HuggingFace Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset conll2003 (/Users/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63f4ebd1bcb7148b1644497336fd74643d4ce70123334431a3c053b7ee4e96ee)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "373aed2aa7f847ee943b8e067939852d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "conll2003 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A single annotated example looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '0',\n",
       " 'tokens': ['EU',\n",
       "  'rejects',\n",
       "  'German',\n",
       "  'call',\n",
       "  'to',\n",
       "  'boycott',\n",
       "  'British',\n",
       "  'lamb',\n",
       "  '.'],\n",
       " 'pos_tags': [22, 42, 16, 21, 35, 37, 16, 21, 7],\n",
       " 'chunk_tags': [11, 21, 11, 12, 21, 22, 11, 12, 0],\n",
       " 'ner_tags': [3, 0, 7, 0, 0, 0, 7, 0, 0]}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Converting to Recon Examples\n",
    "\n",
    "We're interested in the \"tokens\" and the \"ner_tags\". We'll be converting the Integer labels to the standard str labels with the following label map, then converting them into Recon Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.blank(\"en\")\n",
    "conll_labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from spacy.tokens import Doc\n",
    "from recon.operations.tokenization import add_tokens\n",
    "from recon.types import Example, Span, Token\n",
    "\n",
    "\n",
    "def make_recon_examples(dataset: HFDataset, labels_property: str = \"ner_tags\", labels: Optional[List[str]] = None) -> List[Example]:\n",
    "    examples = []\n",
    "    for i, e in enumerate(dataset):\n",
    "        if labels:\n",
    "            tags = [labels[tag_n] for tag_n in e[labels_property]]\n",
    "        else:\n",
    "            tags = e[labels_property]\n",
    "        doc = Doc(nlp.vocab, words=e[\"tokens\"], spaces=[True] * len(e[\"tokens\"]), ents=tags)\n",
    "        spans = [Span(text=ent.text, start=ent.start_char, end=ent.end_char, label=ent.label_) for ent in doc.ents]\n",
    "        tokens = [Token(text=t.text, start=t.idx, end=t.idx + len(t), id=t.i) for t in doc]\n",
    "        examples.append(Example(text=doc.text, spans=spans, tokens=tokens))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = make_recon_examples(conll2003[\"train\"], labels=conll_labels)\n",
    "dev = make_recon_examples(conll2003[\"validation\"], labels=conll_labels)\n",
    "test = make_recon_examples(conll2003[\"test\"], labels=conll_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load a Recon Corpus\n",
    "\n",
    "Now that we have a list of Recon Examples for each of the train/dev/test split, we can run stats and operations on this data and identify problematic examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    EU\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       " rejects \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    German\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " call to boycott \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    British\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">MISC</span>\n",
       "</mark>\n",
       " lamb . </div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train[0].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a Corpus\n",
    "\n",
    "Load the examples into a Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.corpus import Corpus\n",
    "from recon.dataset import Dataset\n",
    "from recon import get_ner_stats, get_entity_coverage\n",
    "from recon.insights import get_label_disparities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method Dataset.__str__ of <recon.dataset.Dataset object at 0x29ef449d0>>\n",
      "<bound method Dataset.__str__ of <recon.dataset.Dataset object at 0x2b6224190>>\n"
     ]
    }
   ],
   "source": [
    "conll2003_corpus = Corpus(Dataset(\"train\", train), Dataset(\"dev\", dev), Dataset(\"test\", test))\n",
    "print(conll2003_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='u.s.', label='LOC', count=157, examples=[]),\n",
       " EntityCoverage(text='germany', label='LOC', count=97, examples=[]),\n",
       " EntityCoverage(text='london', label='LOC', count=82, examples=[]),\n",
       " EntityCoverage(text='australia', label='LOC', count=82, examples=[]),\n",
       " EntityCoverage(text='france', label='LOC', count=80, examples=[]),\n",
       " EntityCoverage(text='russia', label='LOC', count=79, examples=[]),\n",
       " EntityCoverage(text='world cup', label='MISC', count=77, examples=[]),\n",
       " EntityCoverage(text='italy', label='LOC', count=64, examples=[]),\n",
       " EntityCoverage(text='england', label='LOC', count=58, examples=[]),\n",
       " EntityCoverage(text='china', label='LOC', count=58, examples=[])]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = get_entity_coverage(conll2003_corpus.all)\n",
    "\n",
    "ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='clinton', label='PER', count=23, examples=[]),\n",
       " EntityCoverage(text='yeltsin', label='PER', count=19, examples=[]),\n",
       " EntityCoverage(text='wang', label='PER', count=19, examples=[]),\n",
       " EntityCoverage(text='lebed', label='PER', count=18, examples=[]),\n",
       " EntityCoverage(text='arafat', label='PER', count=14, examples=[]),\n",
       " EntityCoverage(text='suu kyi', label='PER', count=14, examples=[]),\n",
       " EntityCoverage(text='edberg', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='albright', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='lara', label='PER', count=12, examples=[]),\n",
       " EntityCoverage(text='dole', label='PER', count=11, examples=[])]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_ec = [e for e in ec if e.label == \"PER\"]\n",
    "\n",
    "per_ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2271"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(per_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: dev\n",
      "Stats: {\n",
      "    \"n_examples\":3251,\n",
      "    \"n_examples_no_entities\":646,\n",
      "    \"n_annotations\":5942,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"PER\":1842,\n",
      "        \"LOC\":1837,\n",
      "        \"ORG\":1341,\n",
      "        \"MISC\":922\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(conll2003_corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{\n",
      "    \"n_examples\":3251,\n",
      "    \"n_examples_no_entities\":646,\n",
      "    \"n_annotations\":5942,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"PER\":1842,\n",
      "        \"LOC\":1837,\n",
      "        \"ORG\":1341,\n",
      "        \"MISC\":922\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "dev\n",
      "{\n",
      "    \"n_examples\":3454,\n",
      "    \"n_examples_no_entities\":698,\n",
      "    \"n_annotations\":5648,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":1668,\n",
      "        \"ORG\":1661,\n",
      "        \"PER\":1617,\n",
      "        \"MISC\":702\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "test\n",
      "{\n",
      "    \"n_examples\":0,\n",
      "    \"n_examples_no_entities\":0,\n",
      "    \"n_annotations\":0,\n",
      "    \"n_annotations_per_type\":{\n",
      "\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "all\n",
      "{\n",
      "    \"n_examples\":6705,\n",
      "    \"n_examples_no_entities\":1344,\n",
      "    \"n_annotations\":11590,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":3505,\n",
      "        \"PER\":3459,\n",
      "        \"ORG\":3002,\n",
      "        \"MISC\":1624\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for name, stats in conll2003_corpus.apply(get_ner_stats, serialize=True).items():\n",
    "    print(name)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'china', 'santiago'}"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_label_disparities(conll2003_corpus.dev, \"PER\", \"LOC\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003_corpus.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
