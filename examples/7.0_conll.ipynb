{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n",
      "Requirement already satisfied: datasets in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages\n",
      "Requirement already satisfied: requests>=2.19.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: pandas in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: pyarrow>=0.17.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: xxhash in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: filelock in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: dill in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from datasets)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests>=2.19.0->datasets)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests>=2.19.0->datasets)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests>=2.19.0->datasets)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests>=2.19.0->datasets)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from pandas->datasets)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from pandas->datasets)\n",
      "Requirement already satisfied: six>=1.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from python-dateutil>=2.6.1->pandas->datasets)\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['men', 'women', 'last'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "res = requests.get(\"https://raw.githubusercontent.com/marcotcr/checklist/master/checklist/data/names.json\")\n",
    "\n",
    "data = res.json()\n",
    "data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading https://files.pythonhosted.org/packages/88/b1/41130a228dd656a1a31ba281598a968320283f48d42782845f6ba567f00b/transformers-4.2.2-py3-none-any.whl (1.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 894kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: packaging in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Collecting sacremoses (from transformers)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: numpy in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Collecting tokenizers==0.9.4 (from transformers)\n",
      "  Downloading https://files.pythonhosted.org/packages/5f/f7/b7a4861a66660ca686df9e50dd3669b006bbe68120cc01f81afe4d76e2cc/tokenizers-0.9.4-cp36-cp36m-manylinux1_x86_64.whl (2.9MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.9MB 541kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: filelock in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from transformers)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from packaging->transformers)\n",
      "Requirement already satisfied: six in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from packaging->transformers)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->transformers)\n",
      "Requirement already satisfied: joblib in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sacremoses->transformers)\n",
      "Requirement already satisfied: click in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sacremoses->transformers)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests->transformers)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests->transformers)\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.9.4 transformers-4.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f54836b93e034ed68a83c5b1d101ea70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'bort'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-204-a1eb3126b467>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAutoModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"stefan-it/bort-full\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/recon/.venv/lib/python3.6/site-packages/transformers/models/auto/modeling_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    723\u001b[0m             config, kwargs = AutoConfig.from_pretrained(\n\u001b[0;32m--> 724\u001b[0;31m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_unused_kwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    725\u001b[0m             )\n\u001b[1;32m    726\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/.venv/lib/python3.6/site-packages/transformers/models/auto/configuration_auto.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"model_type\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mconfig_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m             \u001b[0mconfig_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCONFIG_MAPPING\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model_type\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mconfig_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'bort'"
     ]
    }
   ],
   "source": [
    "model = AutoModel.from_pretrained(\"stefan-it/bort-full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = pipeline(\"ner\", grouped_entities=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Michael works as an ML Engineer.',\n",
       " 'Christopher works as an ML Engineer.',\n",
       " 'Matthew works as an ML Engineer.',\n",
       " 'David works as an ML Engineer.',\n",
       " 'James works as an ML Engineer.']"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = []\n",
    "for name in data[\"men\"]:\n",
    "    \n",
    "    sent = f\"{name} works as an ML Engineer.\"\n",
    "    tests.append(sent)\n",
    "    \n",
    "tests[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "405"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data[\"men\"])\n",
    "\n",
    "data[\"men\"].index(\"Justice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'entity_group': 'PER',\n",
       "  'score': 0.975161075592041,\n",
       "  'word': 'Justice P',\n",
       "  'start': 0,\n",
       "  'end': 9},\n",
       " {'entity_group': 'ORG',\n",
       "  'score': 0.5998892188072205,\n",
       "  'word': '##L',\n",
       "  'start': 23,\n",
       "  'end': 24}]"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp(\"Justice P works as an ML Engineer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Justice works as an ML Engineer.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-191-9c130c6108ed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroup_entities\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: Justice works as an ML Engineer."
     ]
    }
   ],
   "source": [
    "for text in tests:\n",
    "    res = nlp.group_entities(nlp(text))\n",
    "    assert len(res) == 1, text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /home/kabirkhan/.cache/huggingface/datasets/d6f1cdf0f4c7930542df277b4651bd3834621ed5f6748a342cdf00f63e215853.c32530170936ad716612fd9fc895a35cb32ce1fcbeb6b2b82e0be5f5c902ca96.py for additional imports.\n",
      "Found main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2002/conll2002.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2002\n",
      "Found specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2002/conll2002.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2002/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee\n",
      "Found script file from https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2002/conll2002.py to /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2002/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee/conll2002.py\n",
      "Found dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2002/dataset_infos.json to /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2002/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee/dataset_infos.json\n",
      "Found metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2002/conll2002.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2002/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee/conll2002.json\n",
      "Loading Dataset Infos from /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2002/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee\n",
      "Generating dataset conll2002 (/home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conll2002/es (download: 3.95 MiB, generated: 8.87 MiB, post-processed: Unknown size, total: 12.82 MiB) to /home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/esp.train not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/downloads/tmprxlbiztq\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf048d714b3c4faa9a004b251f288d9f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=712623.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/esp.train in cache at /home/kabirkhan/.cache/huggingface/datasets/downloads/3d8f3e234c8ecdfd23c89389c04e034eb0dd6d61e4a57bd8c110170a87680652\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/downloads/3d8f3e234c8ecdfd23c89389c04e034eb0dd6d61e4a57bd8c110170a87680652\n",
      " 33%|███▎      | 1/3 [00:00<00:01,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/esp.testa not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/downloads/tmpoubace33\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5bc7b7f8857b4932ab3c8b722bf0c98b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=141104.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/esp.testa in cache at /home/kabirkhan/.cache/huggingface/datasets/downloads/e75670284be5e2a4d8719f6aff1cdf8b3fd11c13243354e7c0dfb1b1a5683e0e\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/downloads/e75670284be5e2a4d8719f6aff1cdf8b3fd11c13243354e7c0dfb1b1a5683e0e\n",
      " 67%|██████▋   | 2/3 [00:01<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/esp.testb not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/downloads/tmpwo523ax4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b86c738a1ace4fee9a9ec42901ec8819",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=137919.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://raw.githubusercontent.com/teropa/nlp/master/resources/corpora/conll2002/esp.testb in cache at /home/kabirkhan/.cache/huggingface/datasets/downloads/4bb8fa902ae5e10f32475d5f75006cc254696946753954895824f6c718cb3d7b\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/downloads/4bb8fa902ae5e10f32475d5f75006cc254696946753954895824f6c718cb3d7b\n",
      "100%|██████████| 3/3 [00:02<00:00,  1.07it/s]\n",
      "Downloading took 0.0 min\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checksum Computation took 0.0 min\n",
      "100%|██████████| 3/3 [00:00<00:00, 275.48it/s]\n",
      "All the checksums matched successfully for dataset source files\n",
      "Generating split train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 8324 examples in 6672173 bytes /home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee.incomplete/conll2002-train.arrow.\n",
      "Generating split validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1916 examples in 1333784 bytes /home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee.incomplete/conll2002-validation.arrow.\n",
      "Generating split test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 1518 examples in 1294156 bytes /home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee.incomplete/conll2002-test.arrow.\n",
      "All the splits matched successfully.\n",
      "Constructing Dataset for split train, validation, test, from /home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2002 downloaded and prepared to /home/kabirkhan/.cache/huggingface/datasets/conll2002/es/1.0.0/f129dc9284a9ff31962afc9273f8654cd8c199a0de80601203f232a5384d79ee. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 23.31it/s]\n"
     ]
    }
   ],
   "source": [
    "conll2002 = load_dataset(\"conll2002\", \"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.tokens import Doc, Span as SpacySpan\n",
    "from spacy.gold import offsets_from_biluo_tags, spans_from_biluo_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.blank(\"es\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll_labels = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\", \"B-MISC\", \"I-MISC\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_entities(tags):\n",
    "    entities = []\n",
    "    prev_tag = \"O\"\n",
    "    start = None\n",
    "    for i, tag in enumerate(tags):\n",
    "        print(tag)\n",
    "        if tag is None:\n",
    "            continue\n",
    "        if tag.startswith(\"O\"):\n",
    "            # TODO: We shouldn't be getting these malformed inputs. Fix this.\n",
    "            if start is not None:\n",
    "                start = None\n",
    "            continue\n",
    "#         elif tag == \"-\":\n",
    "#             continue\n",
    "#         elif tag.startswith(\"I\"):\n",
    "#             if start is None:\n",
    "#                 raise ValueError(Errors.E067.format(tags=tags[:i + 1]))\n",
    "#             continue\n",
    "        if tag.startswith(\"B\"):\n",
    "            start = i\n",
    "            prev_tag = tag\n",
    "        elif prev_tag != \"O\" and prev_tag == tag:\n",
    "            continue\n",
    "        elif prev_tag != \"O\" and tag == \"O\":\n",
    "            entities.append((tag[2:], start, i))\n",
    "            start = None\n",
    "#         elif tag.startswith(\"L\"):\n",
    "#             entities.append((tag[2:], start, i))\n",
    "#             start = None\n",
    "        else:\n",
    "            raise ValueError(Errors.E068.format(tag=tag))\n",
    "        prev_tag = tag\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tags_to_entities(tags):\n",
    "    entities = []\n",
    "    start = None\n",
    "    prev_tag = \"O\"\n",
    "    for i, tag in enumerate(tags):\n",
    "        if tag is None:\n",
    "            continue\n",
    "        if tag.startswith(\"I\"):\n",
    "            if start is None:\n",
    "                if i == 0:\n",
    "                    start = 0\n",
    "                else:\n",
    "                    raise\n",
    "            continue\n",
    "        elif tag.startswith(\"B\"):\n",
    "            start = i\n",
    "        elif (prev_tag.startswith(\"I\") or prev_tag.startswith(\"B\")) and tag != \"I\":\n",
    "            entities.append((prev_tag[2:], start, i - 1))\n",
    "            start = None\n",
    "        else:\n",
    "            start = None\n",
    "        prev_tag = tag\n",
    "        \n",
    "    if start:\n",
    "        entities.append((tag[2:], start, i))\n",
    "    return entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spans_from_bio_tags(doc, tags):\n",
    "    \"\"\"Encode per-token tags following the BILUO scheme into Span object, e.g.\n",
    "    to overwrite the doc.ents.\n",
    "    doc (Doc): The document that the BILUO tags refer to.\n",
    "    entities (iterable): A sequence of BILUO tags with each tag describing one\n",
    "        token. Each tags string will be of the form of either \"\", \"O\" or\n",
    "        \"{action}-{label}\", where action is one of \"B\", \"I\", \"L\", \"U\".\n",
    "    RETURNS (list): A sequence of Span objects.\n",
    "    \"\"\"\n",
    "    token_offsets = tags_to_entities(tags)\n",
    "    spans = []\n",
    "    for label, start_idx, end_idx in token_offsets:\n",
    "        span = SpacySpan(doc, start_idx, end_idx + 1, label=label)\n",
    "        spans.append(span)\n",
    "    return spans\n",
    "\n",
    "def offsets_from_bio_tags(doc, tags):\n",
    "    \"\"\"Encode per-token tags following the BILUO scheme into entity offsets.\n",
    "    doc (Doc): The document that the BILUO tags refer to.\n",
    "    entities (iterable): A sequence of BILUO tags with each tag describing one\n",
    "        token. Each tags string will be of the form of either \"\", \"O\" or\n",
    "        \"{action}-{label}\", where action is one of \"B\", \"I\", \"L\", \"U\".\n",
    "    RETURNS (list): A sequence of `(start, end, label)` triples. `start` and\n",
    "        `end` will be character-offset integers denoting the slice into the\n",
    "        original string.\n",
    "    \"\"\"\n",
    "    spans = spans_from_bio_tags(doc, tags)\n",
    "    return [(span.start_char, span.end_char, span.label_) for span in spans]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.types import Example, Span"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'validation', 'test'])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2002.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_recon_examples(dataset, labels=None, labels_property = \"ner_tags\"):\n",
    "\n",
    "    examples = []\n",
    "\n",
    "    for i, e in enumerate(dataset):\n",
    "        doc = Doc(nlp.vocab, words=e[\"tokens\"], spaces=[True] * len(e[\"tokens\"]))\n",
    "        \n",
    "        if labels:\n",
    "            tags = [labels[tag_n] for tag_n in e[labels_property]]\n",
    "        else:\n",
    "            tags = e[labels_property]\n",
    "        try:\n",
    "            offsets = offsets_from_bio_tags(doc, tags)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"ERROR AT INDEX {i}\")\n",
    "            print(tags)\n",
    "\n",
    "\n",
    "        spans = [Span(text=doc.text[e[0]:e[1]], start=e[0], end=e[1], label=e[2]) for e in offsets]\n",
    "        examples.append(Example(text=doc.text, spans=spans))\n",
    "\n",
    "    return examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = make_recon_examples(conll2002[\"train\"], conll_labels)\n",
    "dev = make_recon_examples(conll2002[\"validation\"], conll_labels)\n",
    "test = make_recon_examples(conll2002[\"test\"], conll_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.corpus import Corpus\n",
    "from recon.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2002_corpus = Corpus(Dataset(\"train\", train), Dataset(\"dev\", dev), Dataset(\"test\", test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.stats import get_ner_stats, get_entity_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='efe', label='ORG', count=389, examples=[]),\n",
       " EntityCoverage(text='gobierno', label='ORG', count=96, examples=[]),\n",
       " EntityCoverage(text='madrid', label='LOC', count=75, examples=[]),\n",
       " EntityCoverage(text='españa', label='LOC', count=70, examples=[]),\n",
       " EntityCoverage(text='efecom', label='ORG', count=46, examples=[]),\n",
       " EntityCoverage(text='pp', label='ORG', count=40, examples=[]),\n",
       " EntityCoverage(text='nueva york', label='LOC', count=36, examples=[]),\n",
       " EntityCoverage(text='sevilla', label='LOC', count=32, examples=[]),\n",
       " EntityCoverage(text='barcelona', label='ORG', count=31, examples=[]),\n",
       " EntityCoverage(text='brasil', label='LOC', count=29, examples=[])]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = get_entity_coverage(conll2002_corpus.all)\n",
    "\n",
    "ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='roberto baggio', label='PER', count=18, examples=[]),\n",
       " EntityCoverage(text='rivaldo', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='chávez', label='PER', count=11, examples=[]),\n",
       " EntityCoverage(text='josé maría aznar', label='PER', count=11, examples=[]),\n",
       " EntityCoverage(text='franca', label='PER', count=10, examples=[]),\n",
       " EntityCoverage(text='herrera', label='PER', count=9, examples=[]),\n",
       " EntityCoverage(text='castaño', label='PER', count=8, examples=[]),\n",
       " EntityCoverage(text='gonzález', label='PER', count=8, examples=[]),\n",
       " EntityCoverage(text='pinochet', label='PER', count=8, examples=[]),\n",
       " EntityCoverage(text='buffon', label='PER', count=8, examples=[])]"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "per_ec = [e for e in ec if e.label == \"PER\"]\n",
    "\n",
    "per_ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1242"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(per_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Checking /home/kabirkhan/.cache/huggingface/datasets/8d3f4896bd3f278d2fcd9f077e0ddfb793f9d6f78f698895912939aa05b8210f.c5253489aa96f4f1edcf06762a3bf3eb7d4db44e87bdf51f61f2c7a87f0581e3.py for additional imports.\n",
      "Found main folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wnut_17/wnut_17.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/wnut_17\n",
      "Found specific version folder for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wnut_17/wnut_17.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc\n",
      "Found script file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wnut_17/wnut_17.py to /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc/wnut_17.py\n",
      "Found dataset infos file from https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wnut_17/dataset_infos.json to /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc/dataset_infos.json\n",
      "Found metadata file for dataset https://raw.githubusercontent.com/huggingface/datasets/1.0.1/datasets/wnut_17/wnut_17.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc/wnut_17.json\n",
      "No config specified, defaulting to first: wnut_17/wnut_17\n",
      "Loading Dataset Infos from /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/wnut_17/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc\n",
      "Overwrite dataset info from restored data version.\n",
      "Loading Dataset info from /home/kabirkhan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc\n",
      "Reusing dataset wnut_17 (/home/kabirkhan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc)\n",
      "Constructing Dataset for split train, validation, test, from /home/kabirkhan/.cache/huggingface/datasets/wnut_17/wnut_17/1.0.0/1a50e23b62862f940a360c6f6f8f9c209c26943af31c5adf1b35d3846e389edc\n",
      "100%|██████████| 3/3 [00:00<00:00, 20.16it/s]\n"
     ]
    }
   ],
   "source": [
    "wnut17 = load_dataset(\"wnut_17\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (3394, 3), 'validation': (1009, 3), 'test': (1287, 3)}"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wnut17.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '4',\n",
       " 'labels': ['B-person', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O'],\n",
       " 'tokens': ['4Dbling',\n",
       "  \"'s\",\n",
       "  'place',\n",
       "  'til',\n",
       "  'monday',\n",
       "  ',',\n",
       "  'party',\n",
       "  'party',\n",
       "  'party',\n",
       "  '.',\n",
       "  '&lt;',\n",
       "  '3']}"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17_train = make_recon_examples(wnut17[\"train\"], None, labels_property=\"labels\")\n",
    "wnut17_dev = make_recon_examples(wnut17[\"validation\"], None, labels_property=\"labels\")\n",
    "wnut17_test = make_recon_examples(wnut17[\"test\"], None, labels_property=\"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnut17_corpus = Corpus(Dataset(\"train\", wnut17_train), Dataset(\"dev\", wnut17_dev), Dataset(\"test\", wnut17_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='trump', label='person', count=46, examples=[]),\n",
       " EntityCoverage(text='ireland', label='location', count=8, examples=[]),\n",
       " EntityCoverage(text='casey', label='person', count=6, examples=[]),\n",
       " EntityCoverage(text='europe', label='location', count=6, examples=[]),\n",
       " EntityCoverage(text='tanner', label='person', count=6, examples=[]),\n",
       " EntityCoverage(text='democrats', label='group', count=5, examples=[]),\n",
       " EntityCoverage(text='calgary', label='location', count=5, examples=[]),\n",
       " EntityCoverage(text='ryan', label='person', count=4, examples=[]),\n",
       " EntityCoverage(text='shaq', label='person', count=4, examples=[]),\n",
       " EntityCoverage(text='bill', label='person', count=4, examples=[])]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = get_entity_coverage(wnut17_corpus.all)\n",
    "ec[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/conll2003.py not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/tmp_g87rbg9\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7aef0763a2a4afca57ad14fe0871d72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=9483.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/conll2003.py in cache at /home/kabirkhan/.cache/huggingface/datasets/dfc61f206213471d08da0b3f6b244fda2969844536ea1142ac184dd8a2cec330.208b18e334e6cb60d38d497ec39f408036a7aa650621a32891d2e4705b9ad912.py\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/dfc61f206213471d08da0b3f6b244fda2969844536ea1142ac184dd8a2cec330.208b18e334e6cb60d38d497ec39f408036a7aa650621a32891d2e4705b9ad912.py\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/dataset_infos.json not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/tmpbaxsr5gu\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bafa20517baf4632a0a9383c08761fb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=4177.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/dataset_infos.json in cache at /home/kabirkhan/.cache/huggingface/datasets/79d6da4a44dd6c6a779fc49f39f5bd598d31e3ac09a6ee8601d1aaf8906b20ea.ee3785ff90393d9179a42f6eddd4633e7efc825032e2b7d4e637e5cea8466df9\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/79d6da4a44dd6c6a779fc49f39f5bd598d31e3ac09a6ee8601d1aaf8906b20ea.ee3785ff90393d9179a42f6eddd4633e7efc825032e2b7d4e637e5cea8466df9\n",
      "Checking /home/kabirkhan/.cache/huggingface/datasets/dfc61f206213471d08da0b3f6b244fda2969844536ea1142ac184dd8a2cec330.208b18e334e6cb60d38d497ec39f408036a7aa650621a32891d2e4705b9ad912.py for additional imports.\n",
      "Creating main folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/conll2003.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2003\n",
      "Creating specific version folder for dataset https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/conll2003.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664\n",
      "Copying script file from https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/conll2003.py to /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664/conll2003.py\n",
      "Copying dataset infos file from https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/dataset_infos.json to /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664/dataset_infos.json\n",
      "Creating metadata file for dataset https://s3.amazonaws.com/datasets.huggingface.co/datasets/datasets/conll2003/conll2003.py at /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664/conll2003.json\n",
      "No config specified, defaulting to first: conll2003/conll2003\n",
      "Loading Dataset Infos from /home/kabirkhan/.cache/huggingface/modules/datasets_modules/datasets/conll2003/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664\n",
      "Generating dataset conll2003 (/home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset conll2003/conll2003 (download: 4.63 MiB, generated: 9.78 MiB, post-processed: Unknown size, total: 14.41 MiB) to /home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset not on Hf google storage. Downloading and preparing it from source\n",
      "  0%|          | 0/3 [00:00<?, ?it/s]https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/downloads/tmpvbf3rwrg\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7072c79a81484e9db14d9907618be435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=649539.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/train.txt in cache at /home/kabirkhan/.cache/huggingface/datasets/downloads/07144cf16b7a7a911e3a0944fafe476248258d83a4d36783d968885f345571ee\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/downloads/07144cf16b7a7a911e3a0944fafe476248258d83a4d36783d968885f345571ee\n",
      " 33%|███▎      | 1/3 [00:02<00:04,  2.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/valid.txt not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/downloads/tmp48jmhjdn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee9d6d621db6431f93f783c04c6293a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=162714.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/valid.txt in cache at /home/kabirkhan/.cache/huggingface/datasets/downloads/82da4e00f7d3318b5628fc5f95ab0e5b314bd92a8ee939be387a64ea68da06c2\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/downloads/82da4e00f7d3318b5628fc5f95ab0e5b314bd92a8ee939be387a64ea68da06c2\n",
      " 67%|██████▋   | 2/3 [00:04<00:02,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/test.txt not found in cache or force_download set to True, downloading to /home/kabirkhan/.cache/huggingface/datasets/downloads/tmpxn2y1mni\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e45ab548b5d94b20b2981106ef7a3b1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=145897.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "storing https://github.com/davidsbatista/NER-datasets/raw/master/CONLL2003/test.txt in cache at /home/kabirkhan/.cache/huggingface/datasets/downloads/4d7b6be967231c656a4ce471ef1de9e3b7804499daac36a131618cf206527529\n",
      "creating metadata file for /home/kabirkhan/.cache/huggingface/datasets/downloads/4d7b6be967231c656a4ce471ef1de9e3b7804499daac36a131618cf206527529\n",
      "100%|██████████| 3/3 [00:05<00:00,  2.00s/it]\n",
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "100%|██████████| 3/3 [00:00<00:00, 234.94it/s]\n",
      "All the checksums matched successfully for dataset source files\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating split train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 14041 examples in 6931393 bytes /home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664.incomplete/conll2003-train.arrow.\n",
      "Generating split validation\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 3250 examples in 1739247 bytes /home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664.incomplete/conll2003-validation.arrow.\n",
      "Generating split test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done writing 3453 examples in 1582078 bytes /home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664.incomplete/conll2003-test.arrow.\n",
      "All the splits matched successfully.\n",
      "Constructing Dataset for split train, validation, test, from /home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664\n",
      "100%|██████████| 3/3 [00:00<00:00, 21.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conll2003 downloaded and prepared to /home/kabirkhan/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/63ba56944e35c1943434322a07ceefd79864672041b7834583709af4a5de4664. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "conll2003 = load_dataset(\"conll2003\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': (14041, 5), 'validation': (3250, 5), 'test': (3453, 5)}"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14041, 5)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conll2003[\"train\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003_train = make_recon_examples(conll2003[\"train\"], labels=conll_labels)\n",
    "conll2003_dev = make_recon_examples(conll2003[\"validation\"], labels=conll_labels)\n",
    "conll2003_test = make_recon_examples(conll2003[\"test\"], labels=conll_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "conll2003_corpus = Corpus(Dataset(\"train\", conll2003_train), Dataset(\"dev\", conll2003_dev), Dataset(\"test\", conll2003_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "ec = get_entity_coverage(conll2003_corpus.all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='clinton', label='PER', count=23, examples=[]),\n",
       " EntityCoverage(text='yeltsin', label='PER', count=19, examples=[]),\n",
       " EntityCoverage(text='wang', label='PER', count=19, examples=[]),\n",
       " EntityCoverage(text='lebed', label='PER', count=17, examples=[]),\n",
       " EntityCoverage(text='arafat', label='PER', count=14, examples=[]),\n",
       " EntityCoverage(text='suu kyi', label='PER', count=14, examples=[]),\n",
       " EntityCoverage(text='edberg', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='albright', label='PER', count=13, examples=[]),\n",
       " EntityCoverage(text='lara', label='PER', count=12, examples=[]),\n",
       " EntityCoverage(text='dole', label='PER', count=11, examples=[])]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[e for e in ec if e.label == \"PER\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n",
      "{\n",
      "    \"n_examples\":3250,\n",
      "    \"n_examples_no_entities\":701,\n",
      "    \"n_annotations\":5842,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":1826,\n",
      "        \"PER\":1816,\n",
      "        \"ORG\":1332,\n",
      "        \"MISC\":868\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "dev\n",
      "{\n",
      "    \"n_examples\":3453,\n",
      "    \"n_examples_no_entities\":737,\n",
      "    \"n_annotations\":5553,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":1651,\n",
      "        \"ORG\":1644,\n",
      "        \"PER\":1591,\n",
      "        \"MISC\":667\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "test\n",
      "{\n",
      "    \"n_examples\":0,\n",
      "    \"n_examples_no_entities\":0,\n",
      "    \"n_annotations\":0,\n",
      "    \"n_annotations_per_type\":{\n",
      "\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "all\n",
      "{\n",
      "    \"n_examples\":6703,\n",
      "    \"n_examples_no_entities\":1438,\n",
      "    \"n_annotations\":11395,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"LOC\":3477,\n",
      "        \"PER\":3407,\n",
      "        \"ORG\":2976,\n",
      "        \"MISC\":1535\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "for name, stats in conll2003_corpus.apply(get_ner_stats, serialize=True).items():\n",
    "    print(name)\n",
    "    print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.25.3'"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached https://files.pythonhosted.org/packages/c3/e2/00cacecafbab071c787019f00ad84ca3185952f6bb9bca9550ed83870d4d/pandas-1.1.5-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already up-to-date: python-dateutil>=2.7.3 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from pandas)\n",
      "Collecting pytz>=2017.2 (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/89/06/2c2d3034b4d6bf22f2a4ae546d16925898658a33b4400cfb7e2c1e2871a3/pytz-2020.5-py2.py3-none-any.whl (510kB)\n",
      "\u001b[K    100% |████████████████████████████████| 512kB 897kB/s eta 0:00:01\n",
      "\u001b[?25hCollecting numpy>=1.15.4 (from pandas)\n",
      "  Downloading https://files.pythonhosted.org/packages/45/b2/6c7545bb7a38754d63048c7696804a0d947328125d81bf12beaa692c3ae3/numpy-1.19.5-cp36-cp36m-manylinux1_x86_64.whl (13.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 13.4MB 38kB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already up-to-date: six>=1.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from python-dateutil>=2.7.3->pandas)\n",
      "Installing collected packages: pytz, numpy, pandas\n",
      "  Found existing installation: pytz 2020.1\n",
      "    Uninstalling pytz-2020.1:\n",
      "      Successfully uninstalled pytz-2020.1\n",
      "  Found existing installation: numpy 1.19.1\n",
      "    Uninstalling numpy-1.19.1:\n",
      "      Successfully uninstalled numpy-1.19.1\n",
      "  Found existing installation: pandas 0.25.3\n",
      "    Uninstalling pandas-0.25.3:\n",
      "      Successfully uninstalled pandas-0.25.3\n",
      "Successfully installed numpy-1.19.5 pandas-1.1.5 pytz-2020.5\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
