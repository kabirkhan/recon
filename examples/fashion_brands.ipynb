{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'version': 1, 'disable_existing_loggers': False, 'formatters': {'default': {'()': 'uvicorn.logging.DefaultFormatter', 'fmt': '%(levelprefix)s %(message)s', 'use_colors': None}, 'access': {'()': 'uvicorn.logging.AccessFormatter', 'fmt': '%(levelprefix)s %(client_addr)s - \"%(request_line)s\" %(status_code)s'}}, 'handlers': {'default': {'formatter': 'default', 'class': 'logging.StreamHandler', 'stream': 'ext://sys.stderr'}, 'access': {'formatter': 'access', 'class': 'logging.StreamHandler', 'stream': 'ext://sys.stdout'}}, 'loggers': {'uvicorn': {'handlers': ['default'], 'level': 'INFO'}, 'uvicorn.error': {'level': 'INFO'}, 'uvicorn.access': {'handlers': ['access'], 'level': 'INFO', 'propagate': False}}}\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "import spacy\n",
    "import srsly\n",
    "# import recon\n",
    "from recon.corpus import Corpus\n",
    "from recon.constants import NONE\n",
    "from recon.corrections import fix_annotations\n",
    "from recon.dataset import Dataset\n",
    "from recon.loaders import read_jsonl\n",
    "from recon.types import Correction, Example, PredictionError, HardestExample, NERStats, EntityCoverageStats, EntityCoverage, Transformation, TransformationType, OperationState\n",
    "from recon.stats import (\n",
    "    get_ner_stats, get_entity_coverage, get_sorted_type_counts, get_probs_from_counts, entropy,\n",
    "    calculate_entity_coverage_entropy, calculate_label_balance_entropy, calculate_label_distribution_similarity,\n",
    "    detect_outliers\n",
    ")\n",
    "import recon.tokenization as tokenization\n",
    "from recon.insights import get_ents_by_label, get_label_disparities, top_prediction_errors, top_label_disparities, get_hardest_examples\n",
    "from recon.recognizer import SpacyEntityRecognizer\n",
    "from recon.operations import registry\n",
    "from recon.store import ExampleStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix Dataset loading with different file names\n",
    "# train = Dataset(\"train\").from_disk(\"./data/fashion_brands/fashion_brands_training.jsonl\")\n",
    "# dev = Dataset(\"dev\").from_disk(\"./data/fashion_brands/fashion_brands_eval.jsonl\")\n",
    "\n",
    "corpus = Corpus.from_disk(\"./data/fashion_brands/\", \"fashion_brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='Nike', label='FASHION_BRAND', count=18, examples=[]),\n",
       " EntityCoverage(text='Uniqlo', label='FASHION_BRAND', count=18, examples=[]),\n",
       " EntityCoverage(text='Bonobos', label='FASHION_BRAND', count=12, examples=[]),\n",
       " EntityCoverage(text='Madewell', label='FASHION_BRAND', count=10, examples=[]),\n",
       " EntityCoverage(text='Allen Edmonds', label='FASHION_BRAND', count=9, examples=[])]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = corpus.apply(get_entity_coverage, case_sensitive=True)\n",
    "ec.all[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ec = {e.text for e in ec.train}\n",
    "len(unique_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.apparelsearch.com/wholesale_clothing/popular_brand_names_clothes.htm\n",
    "extra_brands = [\"Adidas\", \"Aeffe S.P.A\", \"Agatha\", \"Agnes B\", \"\", \"Anna Osmushkina\", \"Anna Sui\", \"Aquascutum\", \"Armani Exchange\", \"Austin Reed\", \"Avirex\", \"BCBG\", \"Benetton\", \"Bisou-Bisou\", \"Body Glove\", \"Bogner\", \"Burton\", \"Brioni\", \"Calvin Klein\", \"Cesarani\", \"Champion\", \"Chanel\", \"Christian Dior\", \"\", \"Christian Lacoix\", \"Claiborne\", \"Club Monaco\", \"Columbia\", \"Converse\", \"Courrages\", \"Cutter & \", \"Buck\", \"Diesel\", \"Dockers\", \"\", \"Dolce & Gabbana\", \"Donna Karan\", \"Ecco\", \"Ecko\", \"Eddie Bauer\", \"Ellesse\", \"Eliott & \", \"Lucca\", \"Energie\", \"Esprit\", \"Everlast\", \"Fia Miami\", \"Fila\", \"Fiorelli\", \"\", \"Fratelli Corneliani\", \"Fred Perry\", \"Fruit of the \", \"Loom\", \"Fubu\", \"\", \"Gianfranco Ferre\", \"Gianni Versace\", \"Giorgio Armani\", \"Gucci\", \"Guess\", \"Helly Hansen\", \"Hugo Boss\", \"J. Crew\", \"Izod\", \"Jitrois\", \"Jennifer Lopez\", \"\", \"Jenny Yoo\", \"Jhane Barnes\", \"Joe Boxer\", \"\", \"John Smedley\", \"Jordache\", \"Kenneth Cole \", \"/ Reaction\", \"Lacoste\", \"Land's End\", \"\", \"La Perla\", \"Laura Ashley\", \"Lee\", \"Le Tigre\", \"Levi's\", \"Liz Claiborne\", \"L.L Bean\", \"\", \"Louis Feraud\", \"Lucky Brand \", \"Jeans\", \"\", \"Madeleine Vionnet\", \"Mango\", \"Marc Jacobs\", \"\", \"Marcia Grachvogel\", \"\", \"Marianne Alvoni\", \"\", \"Michael Kors\", \"Moschino\", \"Mudd\", \"Munsingwear\", \"Nancy LordNew Balance\", \"Nicole Miller\", \"Nike\", \"\", \"Norma Kamali\", \"Oky-coky\", \"Oilily\", \"\", \"Olivier Strelli\", \"Oneill\", \"OP\", \"\", \"OshKosh B'Gosh\", \"Paul Fredrick\", \"Paul Shark\", \"Paul Smith\", \"\", \"Pelle Pelle\", \"Pepe Jeans\", \"Perry Ellis\", \"\", \"Perry Landhaus\", \"Pierre Cardin\", \"\", \"Pierre Garroudi\", \"Prada\", \"Puma\", \"Quiksilver\", \"Ralph Lauren\", \"Rampage\", \"Red Monkey\", \"Red or Dead\", \"Roberto Angelico\", \"Rocawear\", \"Russell\", \"Savane\", \"Salvatore J. \", \"Cesarani\", \"Sean John\", \"Sinequanone\", \"Sisley\", \"Southpole\", \"Speedo\", \"Steven Alan\", \"Swatch\", \"Timberland\", \"Todd Oldham\", \"Tommy Hilfiger\", \"Van Heusen\", \"Vans\", \"Versace\", \"Vokal\", \"Wrangler\", \"Yves Saint \", \"Laurent\", \"\", \"Z. Cavaricci\", \"Zanetti\", \"Zero\"]\n",
    "extra_brands = {eb for eb in extra_brands if eb != \"\"}\n",
    "len(extra_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ec.update(extra_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from recon.dataset import Dataset\n",
    "from pydantic import root_validator\n",
    "from recon.types import Example, Span, Token\n",
    "import numpy as np\n",
    "from recon.augmentation import augment_example\n",
    "from recon.operations import operation, registry\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "\n",
    "import names\n",
    "from snorkel.augmentation import transformation_function\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_spans(example: Example, span_subs: Dict[Span, str]) -> Example:\n",
    "    \"\"\"Substitute spans in an example. Replaces span text and alters the example text\n",
    "    and span offsets to create a valid example.\n",
    "\n",
    "    Args:\n",
    "        example (Example): Input example\n",
    "        span_subs (Dict[int, str]): Mapping of span hash to a str replacement text\n",
    "\n",
    "    Returns:\n",
    "        Example: Output example with substituted spans\n",
    "    \"\"\"\n",
    "    span_sub_start_counter = 0\n",
    "\n",
    "    new_example_text = example.text\n",
    "    new_example_spans = []\n",
    "    \n",
    "    prev_example_spans = {hash(span) for span in example.spans}\n",
    "    spans = sorted(set(list(span_subs.keys()) + example.spans), key=lambda s: s.start)\n",
    "        \n",
    "    for span in spans:\n",
    "        should_add_span = hash(span) in prev_example_spans\n",
    "        \n",
    "        prev_end = span.end\n",
    "        new_text = span.text\n",
    "\n",
    "        if span in span_subs:\n",
    "            new_text = span_subs[span]\n",
    "            new_start = span.start + span_sub_start_counter\n",
    "            new_end = new_start + len(new_text)\n",
    "\n",
    "            new_example_text = (\n",
    "                new_example_text[: span.start + span_sub_start_counter]\n",
    "                + new_text\n",
    "                + new_example_text[span.end + span_sub_start_counter :]\n",
    "            )\n",
    "\n",
    "            span.text = new_text\n",
    "            span.start = new_start\n",
    "            span.end = new_end\n",
    "            \n",
    "            span_sub_start_counter += new_end - prev_end\n",
    "        else:\n",
    "            span.start += span_sub_start_counter\n",
    "            span.end = span.start + len(new_text)\n",
    "            span_sub_start_counter = span.end - prev_end\n",
    "\n",
    "        span.text = new_text\n",
    "        \n",
    "        if should_add_span:\n",
    "            new_example_spans.append(span)\n",
    "        \n",
    "    example.text = new_example_text\n",
    "    example.spans = new_example_spans\n",
    "\n",
    "    return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def augment_example(\n",
    "    example: Example,\n",
    "    span_f: Callable[[Span, Any], Optional[str]],\n",
    "    spans: List[Span] = None,\n",
    "    span_label: str = None,\n",
    "    **kwargs: Any,\n",
    ") -> List[Example]:\n",
    "\n",
    "    if spans is None:\n",
    "        spans = example.spans\n",
    "\n",
    "    prev_example_hash = hash(example)\n",
    "    example_t = None\n",
    "\n",
    "    if span_label:\n",
    "        spans = [s for s in spans if s.label == span_label]\n",
    "\n",
    "    if spans:\n",
    "        spans_to_sub = [np.random.choice(spans)]\n",
    "\n",
    "        span_subs = {}\n",
    "        for span in spans_to_sub:\n",
    "            res = span_f(span, **kwargs)  #  type: ignore\n",
    "            if res:\n",
    "                span_subs[span] = res\n",
    "\n",
    "        if any(span_subs.values()):\n",
    "            res = substitute_spans(example, span_subs)\n",
    "            if hash(res) != prev_example_hash:\n",
    "                example_t = res\n",
    "\n",
    "    return example_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ent_label_sub(\n",
    "    example: Example, label: str, subs: List[str]\n",
    ") -> List[Example]:\n",
    "    \n",
    "    def augmentation_f(span: Span, subs: List[str]) -> Optional[str]:\n",
    "        subs = [s for s in subs if s != span.text]\n",
    "        sub = None\n",
    "        if len(subs) > 0:\n",
    "            sub = np.random.choice(subs)\n",
    "        return sub\n",
    "\n",
    "    return augment_example(example, span_f=augmentation_f, span_label=label, subs=subs)\n",
    "\n",
    "\n",
    "# replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "\n",
    "@transformation_function()\n",
    "def brand_sub(example: Example):\n",
    "    return ent_label_sub(example.copy(deep=True), label=\"FASHION_BRAND\", subs=list(unique_ec))\n",
    "\n",
    "\n",
    "# @transformation_function()\n",
    "# def person_sub(example: Example):\n",
    "#     return ent_label_sub(example.copy(deep=True), label=\"PERSON\", subs=replacement_names)\n",
    "\n",
    "@transformation_function()\n",
    "def gpe_sub(example: Example):\n",
    "    return ent_label_sub(example.copy(deep=True), label=\"GPE\", subs=[\"Russia\", \"USA\", \"China\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [0]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = [\n",
    "    brand_sub,\n",
    "#     person_sub,\n",
    "    gpe_sub\n",
    "]\n",
    "\n",
    "# np.random.seed(0)\n",
    "\n",
    "# from snorkel.augmentation import ApplyOnePolicy, RandomPolicy\n",
    "\n",
    "# random_policy = RandomPolicy(\n",
    "#     len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    "# )\n",
    "\n",
    "# random_policy.generate_for_example()\n",
    "\n",
    "\n",
    "policy = ApplyOnePolicy()\n",
    "policy.generate_for_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from snorkel.augmentation.apply.core import BaseTFApplier\n",
    "\n",
    "\n",
    "class ReconDatasetTFApplier(BaseTFApplier):\n",
    "    \n",
    "    def __init__(self, tfs, policy, span_label: str = None, sub_prob: float = 0.5):\n",
    "        super().__init__(tfs, policy)\n",
    "        self.span_label = span_label\n",
    "        self.sub_prob = sub_prob\n",
    "    \n",
    "    def _apply_policy_to_data_point(self, x: Example) -> List[Example]:\n",
    "        \n",
    "        x_transformed = set()\n",
    "        for seq in self._policy.generate_for_example():\n",
    "            x_t = x.copy(deep=True)\n",
    "            # Handle empty sequence for `keep_original`\n",
    "            transform_applied = len(seq) == 0\n",
    "            # Apply TFs\n",
    "            for tf_idx in seq:\n",
    "                tf = self._tfs[tf_idx]                \n",
    "                x_t_or_none = tf(x_t)\n",
    "                # Update if transformation was applied\n",
    "                if x_t_or_none is not None:\n",
    "                    transform_applied = True\n",
    "                    x_t = x_t_or_none.copy(deep=True)\n",
    "            # Add example if original or transformations applied\n",
    "            if transform_applied:\n",
    "                x_transformed.add(x_t)\n",
    "        return list(x_transformed)\n",
    "\n",
    "\n",
    "    def apply(self, ds: Dataset, progress_bar: bool = True) -> Dataset:\n",
    "        \n",
    "        @operation(\"recon.v1.augment\")\n",
    "        def augment(example: Example):\n",
    "            transformed_examples = self._apply_policy_to_data_point(example)\n",
    "            return transformed_examples\n",
    "            \n",
    "        ds.apply_(\"recon.v1.augment\")\n",
    "        \n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 8/1235 [00:00<00:17, 72.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Applying operation 'recon.v1.augment' to dataset 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1235/1235 [00:15<00:00, 81.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.augment'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recon.dataset.Dataset at 0x7f4874422b38>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "applier = ReconDatasetTFApplier(tfs, policy)\n",
    "applier.apply(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
