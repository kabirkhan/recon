{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "import spacy\n",
    "import srsly\n",
    "# import recon\n",
    "from recon.corpus import Corpus\n",
    "from recon.constants import NONE\n",
    "from recon.corrections import fix_annotations\n",
    "from recon.dataset import Dataset\n",
    "from recon.loaders import read_jsonl\n",
    "from recon.types import Correction, Example, PredictionError, HardestExample, NERStats, EntityCoverageStats, EntityCoverage, Transformation, TransformationType, OperationState\n",
    "from recon.stats import (\n",
    "    get_ner_stats, get_entity_coverage, get_sorted_type_counts, get_probs_from_counts, entropy,\n",
    "    calculate_entity_coverage_entropy, calculate_label_balance_entropy, calculate_label_distribution_similarity,\n",
    "    detect_outliers\n",
    ")\n",
    "import recon.tokenization as tokenization\n",
    "from recon.insights import get_ents_by_label, get_label_disparities, top_prediction_errors, top_label_disparities, get_hardest_examples\n",
    "from recon.recognizer import SpacyEntityRecognizer\n",
    "from recon.operations import registry\n",
    "from recon.store import ExampleStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix Dataset loading with different file names\n",
    "# train = Dataset(\"train\").from_disk(\"./data/fashion_brands/fashion_brands_training.jsonl\")\n",
    "# dev = Dataset(\"dev\").from_disk(\"./data/fashion_brands/fashion_brands_eval.jsonl\")\n",
    "\n",
    "corpus = Corpus.from_disk(\"./data/fashion_brands/\", \"fashion_brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":1235,\n",
      "    \"n_examples_no_entities\":930,\n",
      "    \"n_annotations\":527,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":527\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":500,\n",
      "    \"n_examples_no_entities\":371,\n",
      "    \"n_annotations\":238,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":238\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":1735,\n",
      "    \"n_examples_no_entities\":1301,\n",
      "    \"n_annotations\":765,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":765\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).dev)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='Nike', label='FASHION_BRAND', count=11, examples=[]),\n",
       " EntityCoverage(text='Uniqlo', label='FASHION_BRAND', count=11, examples=[]),\n",
       " EntityCoverage(text='Madewell', label='FASHION_BRAND', count=8, examples=[]),\n",
       " EntityCoverage(text='Bonobos', label='FASHION_BRAND', count=7, examples=[]),\n",
       " EntityCoverage(text='Gucci', label='FASHION_BRAND', count=7, examples=[])]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = corpus.apply(get_entity_coverage, case_sensitive=True)\n",
    "ec.train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "320"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ec = {e.text for e in ec.train}\n",
    "len(unique_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.apparelsearch.com/wholesale_clothing/popular_brand_names_clothes.htm\n",
    "extra_brands = [\"Adidas\", \"Aeffe S.P.A\", \"Agatha\", \"Agnes B\", \"\", \"Anna Osmushkina\", \"Anna Sui\", \"Aquascutum\", \"Armani Exchange\", \"Austin Reed\", \"Avirex\", \"BCBG\", \"Benetton\", \"Bisou-Bisou\", \"Body Glove\", \"Bogner\", \"Burton\", \"Brioni\", \"Calvin Klein\", \"Cesarani\", \"Champion\", \"Chanel\", \"Christian Dior\", \"\", \"Christian Lacoix\", \"Claiborne\", \"Club Monaco\", \"Columbia\", \"Converse\", \"Courrages\", \"Cutter & \", \"Buck\", \"Diesel\", \"Dockers\", \"\", \"Dolce & Gabbana\", \"Donna Karan\", \"Ecco\", \"Ecko\", \"Eddie Bauer\", \"Ellesse\", \"Eliott & \", \"Lucca\", \"Energie\", \"Esprit\", \"Everlast\", \"Fia Miami\", \"Fila\", \"Fiorelli\", \"\", \"Fratelli Corneliani\", \"Fred Perry\", \"Fruit of the \", \"Loom\", \"Fubu\", \"\", \"Gianfranco Ferre\", \"Gianni Versace\", \"Giorgio Armani\", \"Gucci\", \"Guess\", \"Helly Hansen\", \"Hugo Boss\", \"J. Crew\", \"Izod\", \"Jitrois\", \"Jennifer Lopez\", \"\", \"Jenny Yoo\", \"Jhane Barnes\", \"Joe Boxer\", \"\", \"John Smedley\", \"Jordache\", \"Kenneth Cole \", \"/ Reaction\", \"Lacoste\", \"Land's End\", \"\", \"La Perla\", \"Laura Ashley\", \"Lee\", \"Le Tigre\", \"Levi's\", \"Liz Claiborne\", \"L.L Bean\", \"\", \"Louis Feraud\", \"Lucky Brand \", \"Jeans\", \"\", \"Madeleine Vionnet\", \"Mango\", \"Marc Jacobs\", \"\", \"Marcia Grachvogel\", \"\", \"Marianne Alvoni\", \"\", \"Michael Kors\", \"Moschino\", \"Mudd\", \"Munsingwear\", \"Nancy LordNew Balance\", \"Nicole Miller\", \"Nike\", \"\", \"Norma Kamali\", \"Oky-coky\", \"Oilily\", \"\", \"Olivier Strelli\", \"Oneill\", \"OP\", \"\", \"OshKosh B'Gosh\", \"Paul Fredrick\", \"Paul Shark\", \"Paul Smith\", \"\", \"Pelle Pelle\", \"Pepe Jeans\", \"Perry Ellis\", \"\", \"Perry Landhaus\", \"Pierre Cardin\", \"\", \"Pierre Garroudi\", \"Prada\", \"Puma\", \"Quiksilver\", \"Ralph Lauren\", \"Rampage\", \"Red Monkey\", \"Red or Dead\", \"Roberto Angelico\", \"Rocawear\", \"Russell\", \"Savane\", \"Salvatore J. \", \"Cesarani\", \"Sean John\", \"Sinequanone\", \"Sisley\", \"Southpole\", \"Speedo\", \"Steven Alan\", \"Swatch\", \"Timberland\", \"Todd Oldham\", \"Tommy Hilfiger\", \"Van Heusen\", \"Vans\", \"Versace\", \"Vokal\", \"Wrangler\", \"Yves Saint \", \"Laurent\", \"\", \"Z. Cavaricci\", \"Zanetti\", \"Zero\"]\n",
    "extra_brands = {eb for eb in extra_brands if eb != \"\"}\n",
    "len(extra_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ec.update(extra_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "444"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from recon.dataset import Dataset\n",
    "from pydantic import root_validator\n",
    "from recon.types import Example, Span, Token\n",
    "import numpy as np\n",
    "from recon.augmentation import substitute_spans\n",
    "from recon.operations import operation, registry\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "\n",
    "import names\n",
    "from snorkel.augmentation import transformation_function, ApplyAllPolicy\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_pre = SpacyPreProcessor(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def augment_example(\n",
    "    example: Example,\n",
    "    span_f: Callable[[Span, Any], Optional[str]],\n",
    "    spans: List[Span] = None,\n",
    "    span_label: str = None,\n",
    "    **kwargs: Any,\n",
    ") -> List[Example]:\n",
    "\n",
    "    if spans is None:\n",
    "        spans = example.spans\n",
    "\n",
    "    prev_example_hash = hash(example)\n",
    "    example_t = None\n",
    "\n",
    "    if span_label:\n",
    "        spans = [s for s in spans if s.label == span_label]\n",
    "\n",
    "    if spans:\n",
    "        spans_to_sub = [np.random.choice(spans)]\n",
    "\n",
    "        span_subs = {}\n",
    "        for span in spans_to_sub:\n",
    "            res = span_f(span, **kwargs)  #  type: ignore\n",
    "            if res:\n",
    "                span_subs[span] = res\n",
    "\n",
    "        if any(span_subs.values()):\n",
    "            res = substitute_spans(example, span_subs)\n",
    "            if hash(res) != prev_example_hash:\n",
    "                example_t = res\n",
    "\n",
    "    return example_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ent_label_sub(\n",
    "    example: Example, label: str, subs: List[str]\n",
    ") -> Optional[Example]:\n",
    "    \n",
    "    def augmentation_f(span: Span, subs: List[str]) -> Optional[str]:\n",
    "        subs = [s for s in subs if s != span.text]\n",
    "        sub = None\n",
    "        if len(subs) > 0:\n",
    "            sub = np.random.choice(subs)\n",
    "        return sub\n",
    "\n",
    "    return augment_example(example, span_f=augmentation_f, span_label=label, subs=subs)\n",
    "\n",
    "\n",
    "# replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "\n",
    "@transformation_function()\n",
    "def brand_sub(example: Example):\n",
    "    return ent_label_sub(example.copy(deep=True), label=\"FASHION_BRAND\", subs=list(unique_ec))\n",
    "\n",
    "\n",
    "# @transformation_function()\n",
    "# def person_sub(example: Example):\n",
    "#     return ent_label_sub(example.copy(deep=True), label=\"PERSON\", subs=replacement_names)\n",
    "\n",
    "@transformation_function()\n",
    "def gpe_sub(example: Example):\n",
    "    return ent_label_sub(example.copy(deep=True), label=\"GPE\", subs=[\"Russia\", \"USA\", \"China\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_sub(\n",
    "    example: Example, spans_to_aliases_map: Dict[Span, str]\n",
    ") -> Optional[Example]:\n",
    "    \n",
    "    def augmentation_f(span: Span, spans_to_aliases_map: Dict[Span, List[str]]) -> Optional[str]:\n",
    "        sub = None\n",
    "        if span in spans_to_aliases_map:\n",
    "            aliases = spans_to_aliases_map[span]\n",
    "\n",
    "            if len(aliases) > 0:\n",
    "                rand_alias = np.random.choice(aliases)\n",
    "                index = aliases.index(rand_alias)\n",
    "                del spans_to_aliases_map[span][index]\n",
    "                sub = rand_alias\n",
    "\n",
    "        return sub\n",
    "\n",
    "    return augment_example(example, span_f=augmentation_f, span_label=label, subs=subs)\n",
    "\n",
    "\n",
    "# @transformation_function()\n",
    "# def skills_sub(example: Example):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "# @operation(\"recon.v1.augment.replace_pos_with_synonym\", pre=[spacy_pre])\n",
    "def replace_pos_with_synonym(\n",
    "    example: Example, \n",
    "    pos: str, \n",
    "    synonym_f: Callable[[str], str] = get_synonym\n",
    "):\n",
    "\n",
    "    pos_map = {\n",
    "        \"VERB\": \"v\",\n",
    "        \"NOUN\": \"n\",\n",
    "        \"ADJ\": \"a\"\n",
    "    }\n",
    "    \n",
    "    if pos not in pos_map:\n",
    "        raise ValueError(f\"Argument `pos` of {pos} not in {''.join(pos_map.keys())}\")\n",
    "\n",
    "    doc = example.doc\n",
    "    span_starts = [s.start for s in example.spans]\n",
    "\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    pos_idxs = [i for i, token in enumerate(doc) if token.pos_ == pos and token.idx not in span_starts]\n",
    "    tokens = [doc[idx] for idx in pos_idxs]\n",
    "    spans = [Span(text=token.text, start=token.idx, end=token.idx + len(token.text), label=\"\") for token in tokens]\n",
    "\n",
    "    def augmentation_f(span: Span, synonym_f: Callable[[str], str] = synonym_f) -> Optional[str]:\n",
    "        return synonym_f(span.text)\n",
    "\n",
    "    return augment_example(\n",
    "        example,\n",
    "        augmentation_f,\n",
    "        spans=spans,\n",
    "    )\n",
    "    \n",
    "\n",
    "spacy_pre = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\")    \n",
    "\n",
    "@transformation_function(pre=[spacy_pre])\n",
    "def replace_verb_with_synonym(example: Example):\n",
    "    return replace_pos_with_synonym(example, \"VERB\")\n",
    "\n",
    "@transformation_function(pre=[spacy_pre])\n",
    "def replace_noun_with_synonym(example: Example):\n",
    "    return replace_pos_with_synonym(example, \"NOUN\")\n",
    "\n",
    "@transformation_function(pre=[spacy_pre])\n",
    "def replace_adj_with_synonym(example: Example):\n",
    "    return replace_pos_with_synonym(example, \"ADJ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [0, 3, 1], [0, 3, 3]]"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = [\n",
    "    brand_sub,\n",
    "    replace_verb_with_synonym,\n",
    "    replace_noun_with_synonym,\n",
    "    replace_adj_with_synonym,\n",
    "#     person_sub,\n",
    "#     gpe_sub\n",
    "]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from snorkel.augmentation import ApplyOnePolicy, RandomPolicy\n",
    "\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=3, n_per_original=2, keep_original=True\n",
    ")\n",
    "\n",
    "random_policy.generate_for_example()\n",
    "\n",
    "\n",
    "# policy = ApplyAllPolicy(len(tfs))\n",
    "# policy.generate_for_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from snorkel.augmentation.apply.core import BaseTFApplier\n",
    "\n",
    "\n",
    "class ReconDatasetTFApplier(BaseTFApplier):\n",
    "    \n",
    "    def __init__(self, tfs, policy, span_label: str = None, sub_prob: float = 0.5):\n",
    "        super().__init__(tfs, policy)\n",
    "        self.span_label = span_label\n",
    "        self.sub_prob = sub_prob\n",
    "    \n",
    "    def _apply_policy_to_data_point(self, x: Example) -> List[Example]:\n",
    "        \n",
    "        x_transformed = set()\n",
    "        for seq in self._policy.generate_for_example():\n",
    "            x_t = x.copy(deep=True)\n",
    "            # Handle empty sequence for `keep_original`\n",
    "            transform_applied = len(seq) == 0\n",
    "            # Apply TFs\n",
    "            for tf_idx in seq:\n",
    "                tf = self._tfs[tf_idx]                \n",
    "                x_t_or_none = tf(x_t)\n",
    "                # Update if transformation was applied\n",
    "                if x_t_or_none is not None:\n",
    "                    transform_applied = True\n",
    "                    x_t = x_t_or_none.copy(deep=True)\n",
    "            # Add example if original or transformations applied\n",
    "            if transform_applied:\n",
    "                x_transformed.add(x_t)\n",
    "        return list(x_transformed)\n",
    "\n",
    "\n",
    "    def apply(self, ds: Dataset, progress_bar: bool = True) -> Dataset:\n",
    "        \n",
    "        @operation(\"recon.v1.augment\")\n",
    "        def augment(example: Example):\n",
    "            transformed_examples = self._apply_policy_to_data_point(example)\n",
    "            return transformed_examples\n",
    "            \n",
    "        ds.apply_(\"recon.v1.augment\")\n",
    "        \n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Applying operation 'recon.v1.augment' inplace\n",
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.augment'\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<recon.dataset.Dataset at 0x7f092f7fdb38>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "applier = ReconDatasetTFApplier(tfs, random_policy)\n",
    "applier.apply(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2793"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":2793,\n",
      "    \"n_examples_no_entities\":1960,\n",
      "    \"n_annotations\":1443,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1443\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":500,\n",
      "    \"n_examples_no_entities\":371,\n",
      "    \"n_annotations\":238,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":238\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":3293,\n",
      "    \"n_examples_no_entities\":2331,\n",
      "    \"n_annotations\":1681,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1681\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).dev)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example(text=\"It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still retention it in place\", spans=[], tokens=[Token(text='It', start=0, end=2, id=0), Token(text=\"'s\", start=2, end=4, id=1), Token(text='all', start=5, end=8, id=2), Token(text='preference', start=9, end=19, id=3), Token(text='for', start=20, end=23, id=4), Token(text='which', start=24, end=29, id=5), Token(text='looks', start=30, end=35, id=6), Token(text='better', start=36, end=42, id=7), Token(text=',', start=42, end=43, id=8), Token(text='personally', start=44, end=54, id=9), Token(text='I', start=55, end=56, id=10), Token(text='feel', start=57, end=61, id=11), Token(text='that', start=62, end=66, id=12), Token(text='the', start=67, end=70, id=13), Token(text='more', start=71, end=75, id=14), Token(text='natural', start=76, end=83, id=15), Token(text='the', start=84, end=87, id=16), Token(text='hair', start=88, end=92, id=17), Token(text='looks', start=93, end=98, id=18), Token(text='the', start=99, end=102, id=19), Token(text='better', start=103, end=109, id=20), Token(text='the', start=110, end=113, id=21), Token(text='style', start=114, end=119, id=22), Token(text=',', start=119, end=120, id=23), Token(text='which', start=121, end=126, id=24), Token(text='for', start=127, end=130, id=25), Token(text='me', start=131, end=133, id=26), Token(text='means', start=134, end=139, id=27), Token(text='going', start=140, end=145, id=28), Token(text='with', start=146, end=150, id=29), Token(text='a', start=151, end=152, id=30), Token(text='matte', start=153, end=158, id=31), Token(text='finish', start=159, end=165, id=32), Token(text='which', start=166, end=171, id=33), Token(text='leaves', start=172, end=178, id=34), Token(text='the', start=179, end=182, id=35), Token(text='hair', start=183, end=187, id=36), Token(text='looking', start=188, end=195, id=37), Token(text='as', start=196, end=198, id=38), Token(text='natural', start=199, end=206, id=39), Token(text='as', start=207, end=209, id=40), Token(text='possible', start=210, end=218, id=41), Token(text='while', start=219, end=224, id=42), Token(text='still', start=225, end=230, id=43), Token(text='holding', start=231, end=238, id=44), Token(text='it', start=239, end=241, id=45), Token(text='in', start=242, end=244, id=46), Token(text='place', start=245, end=250, id=47)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1653937787, _task_hash=-1474793941, _session_id=None, doc=It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still holding it in place),\n",
       " Example(text=\"It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still holding it in place\", spans=[], tokens=[Token(text='It', start=0, end=2, id=0), Token(text=\"'s\", start=2, end=4, id=1), Token(text='all', start=5, end=8, id=2), Token(text='preference', start=9, end=19, id=3), Token(text='for', start=20, end=23, id=4), Token(text='which', start=24, end=29, id=5), Token(text='looks', start=30, end=35, id=6), Token(text='better', start=36, end=42, id=7), Token(text=',', start=42, end=43, id=8), Token(text='personally', start=44, end=54, id=9), Token(text='I', start=55, end=56, id=10), Token(text='feel', start=57, end=61, id=11), Token(text='that', start=62, end=66, id=12), Token(text='the', start=67, end=70, id=13), Token(text='more', start=71, end=75, id=14), Token(text='natural', start=76, end=83, id=15), Token(text='the', start=84, end=87, id=16), Token(text='hair', start=88, end=92, id=17), Token(text='looks', start=93, end=98, id=18), Token(text='the', start=99, end=102, id=19), Token(text='better', start=103, end=109, id=20), Token(text='the', start=110, end=113, id=21), Token(text='style', start=114, end=119, id=22), Token(text=',', start=119, end=120, id=23), Token(text='which', start=121, end=126, id=24), Token(text='for', start=127, end=130, id=25), Token(text='me', start=131, end=133, id=26), Token(text='means', start=134, end=139, id=27), Token(text='going', start=140, end=145, id=28), Token(text='with', start=146, end=150, id=29), Token(text='a', start=151, end=152, id=30), Token(text='matte', start=153, end=158, id=31), Token(text='finish', start=159, end=165, id=32), Token(text='which', start=166, end=171, id=33), Token(text='leaves', start=172, end=178, id=34), Token(text='the', start=179, end=182, id=35), Token(text='hair', start=183, end=187, id=36), Token(text='looking', start=188, end=195, id=37), Token(text='as', start=196, end=198, id=38), Token(text='natural', start=199, end=206, id=39), Token(text='as', start=207, end=209, id=40), Token(text='possible', start=210, end=218, id=41), Token(text='while', start=219, end=224, id=42), Token(text='still', start=225, end=230, id=43), Token(text='holding', start=231, end=238, id=44), Token(text='it', start=239, end=241, id=45), Token(text='in', start=242, end=244, id=46), Token(text='place', start=245, end=250, id=47)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1653937787, _task_hash=-1474793941, _session_id=None),\n",
       " Example(text='Heidi boy here, haven’t met another one except for the sales in the SLP store. I’ve only been complemented on my Wyatt’s that I have 5 pairs of, but what does Vancouver know about fashion.', spans=[Span(text='SLP', start=68, end=71, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='n’t', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='’ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='’s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None),\n",
       " Example(text='Heidi boy here, haven’t meet another one except for the sales in the Avirex store. I’ve only been complemented on my Wyatt’s that I have 5 pair of, but what does Vancouver know about fashion.', spans=[Span(text='Avirex', start=69, end=75, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='n’t', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='’ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='’s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None, doc=Heidi boy here, haven’t meet another one except for the sales in the SLP store. I’ve only been complemented on my Wyatt’s that I have 5 pairs of, but what does Vancouver know about fashion.),\n",
       " Example(text='Heidi boy here, haven’t met another one except for the sales in the Brooks Brothers store. I’ve only been complemented on my Wyatt’s that I have 5 pairs of, but what does Vancouver know about fashion.', spans=[Span(text='Brooks Brothers', start=68, end=83, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='n’t', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='’ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='’s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.train_ds.to_disk(\"./fixed_data/fashion_brands_4_augmentations/train\", force=True)\n",
    "\n",
    "raw_data = [e.dict() for e in corpus.train_ds.data]\n",
    "len(raw_data)\n",
    "for e in raw_data:\n",
    "    if 'doc' in e:\n",
    "        del e['doc']\n",
    "\n",
    "srsly.write_jsonl(\"./fixed_data/fashion_brands_4_augmentations/train.jsonl\", raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(text=\"Ooh, that was my shirt! It's Revolve.\", spans=[Span(text='Revolve', start=29, end=36, label='FASHION_BRAND', token_start=9, token_end=9, kb_id=None)], tokens=[Token(text='Ooh', start=0, end=3, id=0), Token(text=',', start=3, end=4, id=1), Token(text='that', start=5, end=9, id=2), Token(text='was', start=10, end=13, id=3), Token(text='my', start=14, end=16, id=4), Token(text='shirt', start=17, end=22, id=5), Token(text='!', start=22, end=23, id=6), Token(text='It', start=24, end=26, id=7), Token(text=\"'s\", start=26, end=28, id=8), Token(text='anachronorm', start=29, end=40, id=9), Token(text='.', start=40, end=41, id=10)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=-187997482, _task_hash=-284841919, _session_id=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.example_store[corpus.train_ds.operations[0].transformations[2].example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":1540,\n",
      "    \"n_examples_no_entities\":930,\n",
      "    \"n_annotations\":1054,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1054\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":500,\n",
      "    \"n_examples_no_entities\":371,\n",
      "    \"n_annotations\":238,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":238\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":2040,\n",
      "    \"n_examples_no_entities\":1301,\n",
      "    \"n_annotations\":1292,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1292\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).dev)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_disk(\"./fixed_data/fashion_brands_ent_label_augment\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
