{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sense2vec\n",
      "Requirement already satisfied: spacy<3.0.0,>=2.2.3 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sense2vec)\n",
      "Requirement already satisfied: srsly>=0.2.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sense2vec)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sense2vec)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sense2vec)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sense2vec)\n",
      "Requirement already satisfied: catalogue>=0.0.4 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from sense2vec)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: setuptools in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->sense2vec)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.0.0,>=2.2.3->sense2vec)\n",
      "Installing collected packages: sense2vec\n",
      "Successfully installed sense2vec-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install sense2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "import spacy\n",
    "import srsly\n",
    "# import recon\n",
    "from recon.corpus import Corpus\n",
    "from recon.constants import NONE\n",
    "from recon.corrections import fix_annotations\n",
    "from recon.dataset import Dataset\n",
    "from recon.loaders import read_jsonl\n",
    "from recon.types import Correction, Example, PredictionError, HardestExample, NERStats, EntityCoverageStats, EntityCoverage, Transformation, TransformationType, OperationState\n",
    "from recon.stats import (\n",
    "    get_ner_stats, get_entity_coverage, get_sorted_type_counts, get_probs_from_counts, entropy,\n",
    "    calculate_entity_coverage_entropy, calculate_label_balance_entropy, calculate_label_distribution_similarity,\n",
    "    detect_outliers\n",
    ")\n",
    "import recon.tokenization as tokenization\n",
    "from recon.insights import get_ents_by_label, get_label_disparities, top_prediction_errors, top_label_disparities, get_hardest_examples\n",
    "from recon.recognizer import SpacyEntityRecognizer\n",
    "from recon.operations import registry\n",
    "from recon.store import ExampleStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Fix Dataset loading with different file names\n",
    "# train = Dataset(\"train\").from_disk(\"./data/fashion_brands/fashion_brands_training.jsonl\")\n",
    "# dev = Dataset(\"dev\").from_disk(\"./data/fashion_brands/fashion_brands_eval.jsonl\")\n",
    "\n",
    "corpus = Corpus.from_disk(\"./data/fashion_brands/\", \"fashion_brands\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":1235,\n",
      "    \"n_examples_no_entities\":930,\n",
      "    \"n_annotations\":527,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":527\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":500,\n",
      "    \"n_examples_no_entities\":371,\n",
      "    \"n_annotations\":238,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":238\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":1735,\n",
      "    \"n_examples_no_entities\":1301,\n",
      "    \"n_annotations\":765,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":765\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).dev)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[EntityCoverage(text='Nike', label='FASHION_BRAND', count=11, examples=[]),\n",
       " EntityCoverage(text='Uniqlo', label='FASHION_BRAND', count=11, examples=[]),\n",
       " EntityCoverage(text='Madewell', label='FASHION_BRAND', count=8, examples=[]),\n",
       " EntityCoverage(text='Bonobos', label='FASHION_BRAND', count=7, examples=[]),\n",
       " EntityCoverage(text='Gucci', label='FASHION_BRAND', count=7, examples=[])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ec = corpus.apply(get_entity_coverage, case_sensitive=True)\n",
    "ec.train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "438"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_ec = {e.text for e in ec.all}\n",
    "len(unique_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "141"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# source: https://www.apparelsearch.com/wholesale_clothing/popular_brand_names_clothes.htm\n",
    "extra_brands = [\"Adidas\", \"Aeffe S.P.A\", \"Agatha\", \"Agnes B\", \"\", \"Anna Osmushkina\", \"Anna Sui\", \"Aquascutum\", \"Armani Exchange\", \"Austin Reed\", \"Avirex\", \"BCBG\", \"Benetton\", \"Bisou-Bisou\", \"Body Glove\", \"Bogner\", \"Burton\", \"Brioni\", \"Calvin Klein\", \"Cesarani\", \"Champion\", \"Chanel\", \"Christian Dior\", \"\", \"Christian Lacoix\", \"Claiborne\", \"Club Monaco\", \"Columbia\", \"Converse\", \"Courrages\", \"Cutter & \", \"Buck\", \"Diesel\", \"Dockers\", \"\", \"Dolce & Gabbana\", \"Donna Karan\", \"Ecco\", \"Ecko\", \"Eddie Bauer\", \"Ellesse\", \"Eliott & \", \"Lucca\", \"Energie\", \"Esprit\", \"Everlast\", \"Fia Miami\", \"Fila\", \"Fiorelli\", \"\", \"Fratelli Corneliani\", \"Fred Perry\", \"Fruit of the \", \"Loom\", \"Fubu\", \"\", \"Gianfranco Ferre\", \"Gianni Versace\", \"Giorgio Armani\", \"Gucci\", \"Guess\", \"Helly Hansen\", \"Hugo Boss\", \"J. Crew\", \"Izod\", \"Jitrois\", \"Jennifer Lopez\", \"\", \"Jenny Yoo\", \"Jhane Barnes\", \"Joe Boxer\", \"\", \"John Smedley\", \"Jordache\", \"Kenneth Cole \", \"/ Reaction\", \"Lacoste\", \"Land's End\", \"\", \"La Perla\", \"Laura Ashley\", \"Lee\", \"Le Tigre\", \"Levi's\", \"Liz Claiborne\", \"L.L Bean\", \"\", \"Louis Feraud\", \"Lucky Brand \", \"Jeans\", \"\", \"Madeleine Vionnet\", \"Mango\", \"Marc Jacobs\", \"\", \"Marcia Grachvogel\", \"\", \"Marianne Alvoni\", \"\", \"Michael Kors\", \"Moschino\", \"Mudd\", \"Munsingwear\", \"Nancy LordNew Balance\", \"Nicole Miller\", \"Nike\", \"\", \"Norma Kamali\", \"Oky-coky\", \"Oilily\", \"\", \"Olivier Strelli\", \"Oneill\", \"OP\", \"\", \"OshKosh B'Gosh\", \"Paul Fredrick\", \"Paul Shark\", \"Paul Smith\", \"\", \"Pelle Pelle\", \"Pepe Jeans\", \"Perry Ellis\", \"\", \"Perry Landhaus\", \"Pierre Cardin\", \"\", \"Pierre Garroudi\", \"Prada\", \"Puma\", \"Quiksilver\", \"Ralph Lauren\", \"Rampage\", \"Red Monkey\", \"Red or Dead\", \"Roberto Angelico\", \"Rocawear\", \"Russell\", \"Savane\", \"Salvatore J. \", \"Cesarani\", \"Sean John\", \"Sinequanone\", \"Sisley\", \"Southpole\", \"Speedo\", \"Steven Alan\", \"Swatch\", \"Timberland\", \"Todd Oldham\", \"Tommy Hilfiger\", \"Van Heusen\", \"Vans\", \"Versace\", \"Vokal\", \"Wrangler\", \"Yves Saint \", \"Laurent\", \"\", \"Z. Cavaricci\", \"Zanetti\", \"Zero\"]\n",
    "extra_brands = {eb for eb in extra_brands if eb != \"\"}\n",
    "len(extra_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_ec.update(extra_brands)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(unique_ec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, List, Optional\n",
    "\n",
    "from recon.dataset import Dataset\n",
    "from pydantic import root_validator\n",
    "from recon.types import Example, Span, Token\n",
    "import numpy as np\n",
    "from recon.augmentation import substitute_spans\n",
    "from recon.operations import operation, registry\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "\n",
    "import names\n",
    "from snorkel.augmentation import transformation_function, ApplyAllPolicy\n",
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_pre = SpacyPreProcessor(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "def augment_example(\n",
    "    example: Example,\n",
    "    span_f: Callable[[Span, Any], Optional[str]],\n",
    "    spans: List[Span] = None,\n",
    "    span_label: str = None,\n",
    "    **kwargs: Any,\n",
    ") -> List[Example]:\n",
    "\n",
    "    if spans is None:\n",
    "        spans = example.spans\n",
    "\n",
    "    prev_example_hash = hash(example)\n",
    "    example_t = None\n",
    "\n",
    "    if span_label:\n",
    "        spans = [s for s in spans if s.label == span_label]\n",
    "\n",
    "    if spans:\n",
    "        spans_to_sub = [np.random.choice(spans)]\n",
    "\n",
    "        span_subs = {}\n",
    "        for span in spans_to_sub:\n",
    "            res = span_f(span, **kwargs)  #  type: ignore\n",
    "            if res:\n",
    "                span_subs[span] = res\n",
    "\n",
    "        if any(span_subs.values()):\n",
    "            res = substitute_spans(example, span_subs)\n",
    "            if hash(res) != prev_example_hash:\n",
    "                example_t = res\n",
    "\n",
    "    return example_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "\n",
    "def ent_label_sub(\n",
    "    example: Example, label: str, subs: List[str]\n",
    ") -> Optional[Example]:\n",
    "    \n",
    "    def augmentation_f(span: Span, subs: List[str]) -> Optional[str]:\n",
    "        subs = [s for s in subs if s != span.text]\n",
    "        sub = None\n",
    "        if len(subs) > 0:\n",
    "            sub = np.random.choice(subs)\n",
    "        return sub\n",
    "\n",
    "    return augment_example(example, span_f=augmentation_f, span_label=label, subs=subs)\n",
    "\n",
    "\n",
    "# replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "\n",
    "@transformation_function()\n",
    "def brand_sub(example: Example):\n",
    "    return ent_label_sub(example.copy(deep=True), label=\"FASHION_BRAND\", subs=list(unique_ec))\n",
    "\n",
    "\n",
    "# @transformation_function()\n",
    "# def person_sub(example: Example):\n",
    "#     return ent_label_sub(example.copy(deep=True), label=\"PERSON\", subs=replacement_names)\n",
    "\n",
    "@transformation_function()\n",
    "def gpe_sub(example: Example):\n",
    "    return ent_label_sub(example.copy(deep=True), label=\"GPE\", subs=[\"Russia\", \"USA\", \"China\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kb_sub(\n",
    "    example: Example, spans_to_aliases_map: Dict[Span, str]\n",
    ") -> Optional[Example]:\n",
    "    \n",
    "    def augmentation_f(span: Span, spans_to_aliases_map: Dict[Span, List[str]]) -> Optional[str]:\n",
    "        sub = None\n",
    "        if span in spans_to_aliases_map:\n",
    "            aliases = spans_to_aliases_map[span]\n",
    "\n",
    "            if len(aliases) > 0:\n",
    "                rand_alias = np.random.choice(aliases)\n",
    "                index = aliases.index(rand_alias)\n",
    "                del spans_to_aliases_map[span][index]\n",
    "                sub = rand_alias\n",
    "\n",
    "        return sub\n",
    "\n",
    "    return augment_example(example, span_f=augmentation_f, span_label=label, subs=subs)\n",
    "\n",
    "\n",
    "# @transformation_function()\n",
    "# def skills_sub(example: Example):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from sense2vec import Sense2VecComponent\n",
    "\n",
    "s2v_path = \"../../sense2vec/s2v_old/\"\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "s2v = Sense2VecComponent(nlp.vocab).from_disk(s2v_path)\n",
    "nlp.add_pipe(s2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(corpus.train[0].text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It\n",
      "None\n",
      "preference\n",
      "[(('personal preference', 'NOUN'), 0.863), (('personal preferences', 'NOUN'), 0.8488), (('preferance', 'NOUN'), 0.8077)]\n",
      "I\n",
      "None\n",
      "more natural the hair\n",
      "None\n",
      "the better the style\n",
      "None\n",
      "me\n",
      "None\n",
      "matte finish\n",
      "[(('satin finish', 'NOUN'), 0.8949), (('glossy', 'ADJ'), 0.8901), (('matte look', 'NOUN'), 0.8879)]\n",
      "hair\n",
      "[(('beard', 'NOUN'), 0.8841), (('curly hair', 'NOUN'), 0.8841), (('stubble', 'NOUN'), 0.8836)]\n",
      "it\n",
      "None\n",
      "place\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "for ph in doc._.s2v_phrases:\n",
    "    try:\n",
    "        most_similar = ph._.s2v_most_similar(3)\n",
    "    except:\n",
    "        most_similar = None\n",
    "    print(ph)\n",
    "    print(most_similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beard'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms = [(('beard', 'NOUN'), 0.8841), (('curly hair', 'NOUN'), 0.8841), (('stubble', 'NOUN'), 0.8836)]\n",
    "ms[0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CACHE\n",
      "{'indices': array([], shape=(0, 0), dtype=float64), 'scores': array([], shape=(0, 0), dtype=float64)}\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-161-ed53b58bfeb8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0ms2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m \u001b[0m_s2v\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_model_most_similar_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms2v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0m_s2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcache\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-161-ed53b58bfeb8>\u001b[0m in \u001b[0;36mtest_model_most_similar_cache\u001b[0;34m(s2v)\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mkey2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ms2v\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mkey1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"honey_bees|NOUN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m     \u001b[0;32massert\u001b[0m \u001b[0mscore1\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mkey2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Beekeepers|NOUN\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32massert\u001b[0m \u001b[0mscore2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3.0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def test_model_most_similar_cache(s2v):\n",
    "    query = \"beekeepers|NOUN\"\n",
    "    s2v.cache = {\"indices\": np.empty((2,1)), \"scores\": np.empty((0,0))}\n",
    "    assert s2v.cache\n",
    "    assert query in s2v\n",
    "    indices = s2v.cache[\"indices\"]\n",
    "    # Modify cache to test that the cache is used and values aren't computed\n",
    "    query_row = s2v.vectors.find(key=s2v.ensure_int_key(query))\n",
    "    scores = np.array(s2v.cache[\"scores\"], copy=True)  # otherwise not writable\n",
    "    honey_bees_row = s2v.vectors.find(key=\"honey_bees|NOUN\")\n",
    "    beekeepers_row = s2v.vectors.find(key=\"Beekeepers|NOUN\")\n",
    "    for i in range(indices.shape[0]):\n",
    "        for j in range(indices.shape[1]):\n",
    "            if indices[i, j] == honey_bees_row:\n",
    "                scores[i, j] = 2.0\n",
    "            elif indices[i, j] == beekeepers_row:\n",
    "                scores[i, j] = 3.0\n",
    "    s2v.cache[\"scores\"] = scores\n",
    "    \n",
    "    print(\"CACHE\")\n",
    "    print(s2v.cache)\n",
    "    ((key1, score1), (key2, score2)) = s2v.most_similar([query], n=2)\n",
    "    assert key1 == \"honey_bees|NOUN\"\n",
    "    assert score1 == 2.0\n",
    "    assert key2 == \"Beekeepers|NOUN\"\n",
    "    assert score2 == 3.0\n",
    "    \n",
    "s2v.s2v.cache = None\n",
    "_s2v = test_model_most_similar_cache(s2v.s2v)\n",
    "_s2v.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "171186"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s2v.s2v.vectors.find(key=\"honey_bees|NOUN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert s2v.s2v.cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1195261"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(s2v.s2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1428648858121"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1195261 * 1195261"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "# @operation(\"recon.v1.augment.replace_pos_with_synonym\", pre=[spacy_pre])\n",
    "def replace_pos_with_synonym(\n",
    "    example: Example, \n",
    "    pos: str, \n",
    "    synonym_f: Callable[[str], str] = get_synonym\n",
    "):\n",
    "\n",
    "    pos_map = {\n",
    "        \"VERB\": \"v\",\n",
    "        \"NOUN\": \"n\",\n",
    "        \"ADJ\": \"a\"\n",
    "    }\n",
    "    \n",
    "    if pos not in pos_map:\n",
    "        raise ValueError(f\"Argument `pos` of {pos} not in {''.join(pos_map.keys())}\")\n",
    "\n",
    "    doc = example.doc\n",
    "    span_starts = [s.start for s in example.spans]\n",
    "\n",
    "    # Get indices of verb tokens in sentence.\n",
    "    pos_idxs = [i for i, token in enumerate(doc) if token.pos_ == pos and token.idx not in span_starts]\n",
    "    tokens = [doc[idx] for idx in pos_idxs]\n",
    "    spans = [Span(text=token.text, start=token.idx, end=token.idx + len(token.text), label=\"\") for token in tokens]\n",
    "\n",
    "    def augmentation_f(span: Span, synonym_f: Callable[[str], str] = synonym_f) -> Optional[str]:\n",
    "        return synonym_f(span.text)\n",
    "\n",
    "    return augment_example(\n",
    "        example,\n",
    "        augmentation_f,\n",
    "        spans=spans,\n",
    "    )\n",
    "    \n",
    "\n",
    "spacy_pre = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\")    \n",
    "\n",
    "@transformation_function(pre=[spacy_pre])\n",
    "def replace_verb_with_synonym(example: Example):\n",
    "    return replace_pos_with_synonym(example, \"VERB\")\n",
    "\n",
    "@transformation_function(pre=[spacy_pre])\n",
    "def replace_noun_with_synonym(example: Example):\n",
    "    return replace_pos_with_synonym(example, \"NOUN\")\n",
    "\n",
    "@transformation_function(pre=[spacy_pre])\n",
    "def replace_adj_with_synonym(example: Example):\n",
    "    return replace_pos_with_synonym(example, \"ADJ\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v_pre = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\")\n",
    "s2v_pre._nlp = nlp\n",
    "\n",
    "\n",
    "@transformation_function(pre=[s2v_pre])\n",
    "def replace_s2v(example):\n",
    "    \n",
    "    doc = example.doc\n",
    "    span_starts = [s.start for s in example.spans]\n",
    "\n",
    "    most_similar_map = {}\n",
    "    for ph in doc._.s2v_phrases:\n",
    "        try:\n",
    "            most_similar = ph._.s2v_most_similar(5)\n",
    "        except:\n",
    "            most_similar = None\n",
    "            \n",
    "        if most_similar:\n",
    "            span = Span(text=ph.text, start=ph.start_char, end=ph.end_char, label=\"\")\n",
    "            if span.start not in span_starts:\n",
    "                most_similar_map[span] = most_similar\n",
    "                \n",
    "    \n",
    "    def augmentation_f(span: Span, most_similar_map: Dict[Span, List[str]]) -> Optional[str]:\n",
    "        choice = np.random.choice(most_similar_map[span][0][0])\n",
    "        return choice\n",
    "\n",
    "    return augment_example(\n",
    "        example,\n",
    "        augmentation_f,\n",
    "        spans=list(most_similar_map.keys()),\n",
    "        most_similar_map=most_similar_map\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [0, 1]]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfs = [\n",
    "    brand_sub,\n",
    "    replace_s2v\n",
    "#     person_sub,\n",
    "#     gpe_sub\n",
    "]\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "from snorkel.augmentation import ApplyOnePolicy, RandomPolicy\n",
    "\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    ")\n",
    "\n",
    "random_policy.generate_for_example()\n",
    "\n",
    "\n",
    "policy = ApplyAllPolicy(len(tfs))\n",
    "policy.generate_for_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from snorkel.augmentation.apply.core import BaseTFApplier\n",
    "\n",
    "\n",
    "class ReconDatasetTFApplier(BaseTFApplier):\n",
    "    \n",
    "    def __init__(self, tfs, policy, span_label: str = None, sub_prob: float = 0.5):\n",
    "        super().__init__(tfs, policy)\n",
    "        self.span_label = span_label\n",
    "        self.sub_prob = sub_prob\n",
    "    \n",
    "    def _apply_policy_to_data_point(self, x: Example) -> List[Example]:\n",
    "        \n",
    "        x_transformed = set()\n",
    "        for seq in self._policy.generate_for_example():\n",
    "            x_t = x.copy(deep=True)\n",
    "            # Handle empty sequence for `keep_original`\n",
    "            transform_applied = len(seq) == 0\n",
    "            # Apply TFs\n",
    "            for tf_idx in seq:\n",
    "                tf = self._tfs[tf_idx]                \n",
    "                x_t_or_none = tf(x_t)\n",
    "                # Update if transformation was applied\n",
    "                if x_t_or_none is not None:\n",
    "                    transform_applied = True\n",
    "                    x_t = x_t_or_none.copy(deep=True)\n",
    "            # Add example if original or transformations applied\n",
    "            if transform_applied:\n",
    "                x_transformed.add(x_t)\n",
    "        return list(x_transformed)\n",
    "\n",
    "\n",
    "    def apply(self, ds: Dataset, progress_bar: bool = True) -> Dataset:\n",
    "        \n",
    "        @operation(\"recon.v1.augment\")\n",
    "        def augment(example: Example):\n",
    "            transformed_examples = self._apply_policy_to_data_point(example)\n",
    "            return transformed_examples\n",
    "            \n",
    "        ds.apply_(\"recon.v1.augment\")\n",
    "        \n",
    "        return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1235"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Applying operation 'recon.v1.augment' to dataset 'train'\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dade5016bb8b41ab89562b34dab31a92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1235.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-27-49911192b610>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mapplier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReconDatasetTFApplier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtfs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mapplier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-25-bc2cabab48d9>\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, ds, progress_bar)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtransformed_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recon.v1.augment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/recon/dataset.py\u001b[0m in \u001b[0;36mapply_\u001b[0;34m(self, operation, initial_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"=> Applying operation '{name}' to dataset '{self.name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mresult\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOperationResult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0mmsg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Completed operation '{name}'\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/recon/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, dataset, *args, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocessed_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpreprocessed_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m                     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-bc2cabab48d9>\u001b[0m in \u001b[0;36maugment\u001b[0;34m(example)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0;34m@\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"recon.v1.augment\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0maugment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mExample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m             \u001b[0mtransformed_examples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply_policy_to_data_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mtransformed_examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-bc2cabab48d9>\u001b[0m in \u001b[0;36m_apply_policy_to_data_point\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     25\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mx_t_or_none\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m                     \u001b[0mtransform_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                     \u001b[0mx_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx_t_or_none\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdeep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m             \u001b[0;31m# Add example if original or transformations applied\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtransform_applied\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/.venv/lib/python3.6/site-packages/pydantic/main.py\u001b[0m in \u001b[0;36mcopy\u001b[0;34m(self, include, exclude, update, deep)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    542\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdeep\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 543\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    544\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    545\u001b[0m         \u001b[0mcls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mcopier\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36m_deepcopy_dict\u001b[0;34m(x, memo, deepcopy)\u001b[0m\n\u001b[1;32m    238\u001b[0m     \u001b[0mmemo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_deepcopy_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/copy.py\u001b[0m in \u001b[0;36mdeepcopy\u001b[0;34m(x, memo, _nil)\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatch_table\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                     \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreductor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                     \u001b[0mreductor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mdoc.pyx\u001b[0m in \u001b[0;36mspacy.tokens.doc.pickle_doc\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/.venv/lib/python3.6/site-packages/srsly/_pickle_api.py\u001b[0m in \u001b[0;36mpickle_dumps\u001b[0;34m(data, protocol)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mRETURNS\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbytest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mThe\u001b[0m \u001b[0mserialized\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mcloudpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/.venv/lib/python3.6/site-packages/srsly/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdumps\u001b[0;34m(obj, protocol)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m         \u001b[0mcp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCloudPickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprotocol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1125\u001b[0;31m         \u001b[0mcp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1126\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetvalue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1127\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/recon/.venv/lib/python3.6/site-packages/srsly/cloudpickle/cloudpickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    480\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minject_addons\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mPickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mRuntimeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'recursion'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36mdump\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    407\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSTOP\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mend_framing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_tuple\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    749\u001b[0m         \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMARK\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    750\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0melement\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 751\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    752\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmemo\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    850\u001b[0m                 \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m                 \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m             \u001b[0;31m# else tmp is empty, and we're done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;31m# Save the reduce() output and finally memoize the object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_reduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mrv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpersistent_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_reduce\u001b[0;34m(self, func, args, state, listitems, dictitems, obj)\u001b[0m\n\u001b[1;32m    632\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m             \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m             \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBUILD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    474\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 476\u001b[0;31m             \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Call unbound method with explicit self\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave_dict\u001b[0;34m(self, obj)\u001b[0m\n\u001b[1;32m    819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    820\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemoize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 821\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_batch_setitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    822\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    823\u001b[0m     \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msave_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36m_batch_setitems\u001b[0;34m(self, items)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtmp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                     \u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSETITEMS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.6/pickle.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, obj, save_persistent_id)\u001b[0m\n\u001b[1;32m    494\u001b[0m             \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce_ex__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m                 \u001b[0mrv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m                 \u001b[0mreduce\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__reduce__\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "applier = ReconDatasetTFApplier(tfs, random_policy)\n",
    "applier.apply(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example(text=\"It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still holding it in place\", spans=[], tokens=[Token(text='It', start=0, end=2, id=0), Token(text=\"'s\", start=2, end=4, id=1), Token(text='all', start=5, end=8, id=2), Token(text='preference', start=9, end=19, id=3), Token(text='for', start=20, end=23, id=4), Token(text='which', start=24, end=29, id=5), Token(text='looks', start=30, end=35, id=6), Token(text='better', start=36, end=42, id=7), Token(text=',', start=42, end=43, id=8), Token(text='personally', start=44, end=54, id=9), Token(text='I', start=55, end=56, id=10), Token(text='feel', start=57, end=61, id=11), Token(text='that', start=62, end=66, id=12), Token(text='the', start=67, end=70, id=13), Token(text='more', start=71, end=75, id=14), Token(text='natural', start=76, end=83, id=15), Token(text='the', start=84, end=87, id=16), Token(text='hair', start=88, end=92, id=17), Token(text='looks', start=93, end=98, id=18), Token(text='the', start=99, end=102, id=19), Token(text='better', start=103, end=109, id=20), Token(text='the', start=110, end=113, id=21), Token(text='style', start=114, end=119, id=22), Token(text=',', start=119, end=120, id=23), Token(text='which', start=121, end=126, id=24), Token(text='for', start=127, end=130, id=25), Token(text='me', start=131, end=133, id=26), Token(text='means', start=134, end=139, id=27), Token(text='going', start=140, end=145, id=28), Token(text='with', start=146, end=150, id=29), Token(text='a', start=151, end=152, id=30), Token(text='matte', start=153, end=158, id=31), Token(text='finish', start=159, end=165, id=32), Token(text='which', start=166, end=171, id=33), Token(text='leaves', start=172, end=178, id=34), Token(text='the', start=179, end=182, id=35), Token(text='hair', start=183, end=187, id=36), Token(text='looking', start=188, end=195, id=37), Token(text='as', start=196, end=198, id=38), Token(text='natural', start=199, end=206, id=39), Token(text='as', start=207, end=209, id=40), Token(text='possible', start=210, end=218, id=41), Token(text='while', start=219, end=224, id=42), Token(text='still', start=225, end=230, id=43), Token(text='holding', start=231, end=238, id=44), Token(text='it', start=239, end=241, id=45), Token(text='in', start=242, end=244, id=46), Token(text='place', start=245, end=250, id=47)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1653937787, _task_hash=-1474793941, _session_id=None),\n",
       " Example(text='Heidi boy here, havent met another one except for the sales in the Christian Dior store. Ive only been complemented on my Wyatts that I have 5 pairs of, but what does Vancouver know about fashion.', spans=[Span(text='Christian Dior', start=68, end=82, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='nt', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None),\n",
       " Example(text='Heidi boy here, havent met another one except for the sales in the SLP store. Ive only been complemented on my Wyatts that I have 5 pairs of, but what does Vancouver know about fashion.', spans=[Span(text='SLP', start=68, end=71, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='nt', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None)]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":2793,\n",
      "    \"n_examples_no_entities\":1960,\n",
      "    \"n_annotations\":1443,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1443\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":500,\n",
      "    \"n_examples_no_entities\":371,\n",
      "    \"n_annotations\":238,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":238\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":3293,\n",
      "    \"n_examples_no_entities\":2331,\n",
      "    \"n_annotations\":1681,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1681\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).dev)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Example(text=\"It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still retention it in place\", spans=[], tokens=[Token(text='It', start=0, end=2, id=0), Token(text=\"'s\", start=2, end=4, id=1), Token(text='all', start=5, end=8, id=2), Token(text='preference', start=9, end=19, id=3), Token(text='for', start=20, end=23, id=4), Token(text='which', start=24, end=29, id=5), Token(text='looks', start=30, end=35, id=6), Token(text='better', start=36, end=42, id=7), Token(text=',', start=42, end=43, id=8), Token(text='personally', start=44, end=54, id=9), Token(text='I', start=55, end=56, id=10), Token(text='feel', start=57, end=61, id=11), Token(text='that', start=62, end=66, id=12), Token(text='the', start=67, end=70, id=13), Token(text='more', start=71, end=75, id=14), Token(text='natural', start=76, end=83, id=15), Token(text='the', start=84, end=87, id=16), Token(text='hair', start=88, end=92, id=17), Token(text='looks', start=93, end=98, id=18), Token(text='the', start=99, end=102, id=19), Token(text='better', start=103, end=109, id=20), Token(text='the', start=110, end=113, id=21), Token(text='style', start=114, end=119, id=22), Token(text=',', start=119, end=120, id=23), Token(text='which', start=121, end=126, id=24), Token(text='for', start=127, end=130, id=25), Token(text='me', start=131, end=133, id=26), Token(text='means', start=134, end=139, id=27), Token(text='going', start=140, end=145, id=28), Token(text='with', start=146, end=150, id=29), Token(text='a', start=151, end=152, id=30), Token(text='matte', start=153, end=158, id=31), Token(text='finish', start=159, end=165, id=32), Token(text='which', start=166, end=171, id=33), Token(text='leaves', start=172, end=178, id=34), Token(text='the', start=179, end=182, id=35), Token(text='hair', start=183, end=187, id=36), Token(text='looking', start=188, end=195, id=37), Token(text='as', start=196, end=198, id=38), Token(text='natural', start=199, end=206, id=39), Token(text='as', start=207, end=209, id=40), Token(text='possible', start=210, end=218, id=41), Token(text='while', start=219, end=224, id=42), Token(text='still', start=225, end=230, id=43), Token(text='holding', start=231, end=238, id=44), Token(text='it', start=239, end=241, id=45), Token(text='in', start=242, end=244, id=46), Token(text='place', start=245, end=250, id=47)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1653937787, _task_hash=-1474793941, _session_id=None, doc=It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still holding it in place),\n",
       " Example(text=\"It's all preference for which looks better, personally I feel that the more natural the hair looks the better the style, which for me means going with a matte finish which leaves the hair looking as natural as possible while still holding it in place\", spans=[], tokens=[Token(text='It', start=0, end=2, id=0), Token(text=\"'s\", start=2, end=4, id=1), Token(text='all', start=5, end=8, id=2), Token(text='preference', start=9, end=19, id=3), Token(text='for', start=20, end=23, id=4), Token(text='which', start=24, end=29, id=5), Token(text='looks', start=30, end=35, id=6), Token(text='better', start=36, end=42, id=7), Token(text=',', start=42, end=43, id=8), Token(text='personally', start=44, end=54, id=9), Token(text='I', start=55, end=56, id=10), Token(text='feel', start=57, end=61, id=11), Token(text='that', start=62, end=66, id=12), Token(text='the', start=67, end=70, id=13), Token(text='more', start=71, end=75, id=14), Token(text='natural', start=76, end=83, id=15), Token(text='the', start=84, end=87, id=16), Token(text='hair', start=88, end=92, id=17), Token(text='looks', start=93, end=98, id=18), Token(text='the', start=99, end=102, id=19), Token(text='better', start=103, end=109, id=20), Token(text='the', start=110, end=113, id=21), Token(text='style', start=114, end=119, id=22), Token(text=',', start=119, end=120, id=23), Token(text='which', start=121, end=126, id=24), Token(text='for', start=127, end=130, id=25), Token(text='me', start=131, end=133, id=26), Token(text='means', start=134, end=139, id=27), Token(text='going', start=140, end=145, id=28), Token(text='with', start=146, end=150, id=29), Token(text='a', start=151, end=152, id=30), Token(text='matte', start=153, end=158, id=31), Token(text='finish', start=159, end=165, id=32), Token(text='which', start=166, end=171, id=33), Token(text='leaves', start=172, end=178, id=34), Token(text='the', start=179, end=182, id=35), Token(text='hair', start=183, end=187, id=36), Token(text='looking', start=188, end=195, id=37), Token(text='as', start=196, end=198, id=38), Token(text='natural', start=199, end=206, id=39), Token(text='as', start=207, end=209, id=40), Token(text='possible', start=210, end=218, id=41), Token(text='while', start=219, end=224, id=42), Token(text='still', start=225, end=230, id=43), Token(text='holding', start=231, end=238, id=44), Token(text='it', start=239, end=241, id=45), Token(text='in', start=242, end=244, id=46), Token(text='place', start=245, end=250, id=47)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1653937787, _task_hash=-1474793941, _session_id=None),\n",
       " Example(text='Heidi boy here, havent met another one except for the sales in the SLP store. Ive only been complemented on my Wyatts that I have 5 pairs of, but what does Vancouver know about fashion.', spans=[Span(text='SLP', start=68, end=71, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='nt', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None),\n",
       " Example(text='Heidi boy here, havent meet another one except for the sales in the Avirex store. Ive only been complemented on my Wyatts that I have 5 pair of, but what does Vancouver know about fashion.', spans=[Span(text='Avirex', start=69, end=75, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='nt', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None, doc=Heidi boy here, havent meet another one except for the sales in the SLP store. Ive only been complemented on my Wyatts that I have 5 pairs of, but what does Vancouver know about fashion.),\n",
       " Example(text='Heidi boy here, havent met another one except for the sales in the Brooks Brothers store. Ive only been complemented on my Wyatts that I have 5 pairs of, but what does Vancouver know about fashion.', spans=[Span(text='Brooks Brothers', start=68, end=83, label='FASHION_BRAND', token_start=15, token_end=15, kb_id=None)], tokens=[Token(text='Heidi', start=0, end=5, id=0), Token(text='boy', start=6, end=9, id=1), Token(text='here', start=10, end=14, id=2), Token(text=',', start=14, end=15, id=3), Token(text='have', start=16, end=20, id=4), Token(text='nt', start=20, end=23, id=5), Token(text='met', start=24, end=27, id=6), Token(text='another', start=28, end=35, id=7), Token(text='one', start=36, end=39, id=8), Token(text='except', start=40, end=46, id=9), Token(text='for', start=47, end=50, id=10), Token(text='the', start=51, end=54, id=11), Token(text='sales', start=55, end=60, id=12), Token(text='in', start=61, end=63, id=13), Token(text='the', start=64, end=67, id=14), Token(text='SLP', start=68, end=71, id=15), Token(text='store', start=72, end=77, id=16), Token(text='.', start=77, end=78, id=17), Token(text='I', start=79, end=80, id=18), Token(text='ve', start=80, end=83, id=19), Token(text='only', start=84, end=88, id=20), Token(text='been', start=89, end=93, id=21), Token(text='complemented', start=94, end=106, id=22), Token(text='on', start=107, end=109, id=23), Token(text='my', start=110, end=112, id=24), Token(text='Wyatt', start=113, end=118, id=25), Token(text='s', start=118, end=120, id=26), Token(text='that', start=121, end=125, id=27), Token(text='I', start=126, end=127, id=28), Token(text='have', start=128, end=132, id=29), Token(text='5', start=133, end=134, id=30), Token(text='pairs', start=135, end=140, id=31), Token(text='of', start=141, end=143, id=32), Token(text=',', start=143, end=144, id=33), Token(text='but', start=145, end=148, id=34), Token(text='what', start=149, end=153, id=35), Token(text='does', start=154, end=158, id=36), Token(text='Vancouver', start=159, end=168, id=37), Token(text='know', start=169, end=173, id=38), Token(text='about', start=174, end=179, id=39), Token(text='fashion', start=180, end=187, id=40), Token(text='.', start=187, end=188, id=41)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=1779771027, _task_hash=1563357463, _session_id=None)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus.train_ds.to_disk(\"./fixed_data/fashion_brands_4_augmentations/train\", force=True)\n",
    "\n",
    "raw_data = [e.dict() for e in corpus.train_ds.data]\n",
    "len(raw_data)\n",
    "for e in raw_data:\n",
    "    if 'doc' in e:\n",
    "        del e['doc']\n",
    "\n",
    "srsly.write_jsonl(\"./fixed_data/fashion_brands_4_augmentations/train.jsonl\", raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(text=\"Ooh, that was my shirt! It's Revolve.\", spans=[Span(text='Revolve', start=29, end=36, label='FASHION_BRAND', token_start=9, token_end=9, kb_id=None)], tokens=[Token(text='Ooh', start=0, end=3, id=0), Token(text=',', start=3, end=4, id=1), Token(text='that', start=5, end=9, id=2), Token(text='was', start=10, end=13, id=3), Token(text='my', start=14, end=16, id=4), Token(text='shirt', start=17, end=22, id=5), Token(text='!', start=22, end=23, id=6), Token(text='It', start=24, end=26, id=7), Token(text=\"'s\", start=26, end=28, id=8), Token(text='anachronorm', start=29, end=40, id=9), Token(text='.', start=40, end=41, id=10)], meta={'section': 'malefashionadvice'}, formatted=True, answer='accept', _view_id='ner_manual', _input_hash=-187997482, _task_hash=-284841919, _session_id=None)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.example_store[corpus.train_ds.operations[0].transformations[2].example]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1540"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus.train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":1540,\n",
      "    \"n_examples_no_entities\":930,\n",
      "    \"n_annotations\":1054,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1054\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":500,\n",
      "    \"n_examples_no_entities\":371,\n",
      "    \"n_annotations\":238,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":238\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n",
      "{\n",
      "    \"n_examples\":2040,\n",
      "    \"n_examples_no_entities\":1301,\n",
      "    \"n_annotations\":1292,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"FASHION_BRAND\":1292\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).dev)\n",
    "print(corpus.apply(get_ner_stats, serialize=True).all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_disk(\"./fixed_data/fashion_brands_ent_label_augment\", force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of recon.operations failed: Traceback (most recent call last):\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 276, in update_instances\n",
      "    refs = gc.get_referrers(old)\n",
      "KeyboardInterrupt\n",
      "]\n",
      "[autoreload of recon.types failed: Traceback (most recent call last):\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 317, in update_class\n",
      "    update_instances(old, new)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 280, in update_instances\n",
      "    ref.__class__ = new\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/pydantic/main.py\", line 345, in __setattr__\n",
      "    raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n",
      "ValueError: \"Span\" object has no field \"__class__\"\n",
      "]\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
