{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /home/kabirkhan/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download(\"wordnet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz (12.0MB)\n",
      "\u001b[K    100% |████████████████████████████████| 12.1MB 52.6MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<2.4.0,>=2.3.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: thinc==7.4.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: setuptools in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1)\n",
      "Installing collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.3.1\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the model via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from types import SimpleNamespace\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "from recon.dataset import Dataset\n",
    "from pydantic import root_validator\n",
    "from recon.types import Example, Span, Token\n",
    "import numpy as np\n",
    "from recon.augmentation import augment_example\n",
    "from recon.operations import operation, registry\n",
    "from recon.preprocess import SpacyPreProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@operation(\"recon.v1.augment.verb_replace\", factory=True)\n",
    "def kb_expansion(\n",
    "    example: Example,\n",
    "    preprocessed_outputs: Dict[str, Any] = {},\n",
    "    n_augs: int = 1,\n",
    "    sub_prob: float = 0.5,\n",
    ") -> List[Example]:\n",
    "\n",
    "    spans_to_aliases_map = preprocessed_outputs[\"recon.v1.span_aliases\"]\n",
    "    print(preprocessed_outputs)\n",
    "    print(example.data)\n",
    "\n",
    "    def augmentation(span: Span, spans_to_aliases_map: Dict[int, List[str]]) -> Optional[str]:\n",
    "        if hash(span) in spans_to_aliases_map:\n",
    "            aliases = spans_to_aliases_map[hash(span)]\n",
    "\n",
    "            if len(aliases) > 0:\n",
    "                rand_alias = np.random.choice(aliases)\n",
    "                index = aliases.index(rand_alias)\n",
    "                del spans_to_aliases_map[hash(span)][index]\n",
    "                return rand_alias\n",
    "\n",
    "    return augment_example(\n",
    "        example,\n",
    "        augmentation,\n",
    "        n_augs=n_augs,\n",
    "        sub_prob=sub_prob,\n",
    "        spans_to_aliases_map=spans_to_aliases_map,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def substitute_spans(example: Example, span_subs: Dict[Span, str]) -> Example:\n",
    "    \"\"\"Substitute spans in an example. Replaces span text and alters the example text\n",
    "    and span offsets to create a valid example.\n",
    "\n",
    "    Args:\n",
    "        example (Example): Input example\n",
    "        span_subs (Dict[int, str]): Mapping of span hash to a str replacement text\n",
    "\n",
    "    Returns:\n",
    "        Example: Output example with substituted spans\n",
    "    \"\"\"\n",
    "    span_sub_start_counter = 0\n",
    "\n",
    "    new_example_text = example.text\n",
    "    new_example_spans = []\n",
    "    \n",
    "    prev_example_spans = {hash(span) for span in example.spans}\n",
    "    spans = sorted(list(span_subs.keys()) + example.spans, key=lambda s: s.start)\n",
    "    \n",
    "    for span in spans:\n",
    "        should_add_span = hash(span) in prev_example_spans\n",
    "        \n",
    "        prev_end = span.end\n",
    "        new_text = span.text\n",
    "\n",
    "        if span in span_subs:\n",
    "            new_text = span_subs[span]\n",
    "            new_start = span.start + span_sub_start_counter\n",
    "            new_end = new_start + len(new_text)\n",
    "\n",
    "            new_example_text = (\n",
    "                new_example_text[: span.start + span_sub_start_counter]\n",
    "                + new_text\n",
    "                + new_example_text[span.end + span_sub_start_counter :]\n",
    "            )\n",
    "\n",
    "            span.text = new_text\n",
    "            span.start = new_start\n",
    "            span.end = new_end\n",
    "            \n",
    "            span_sub_start_counter += new_end - prev_end\n",
    "        else:\n",
    "            span.start += span_sub_start_counter\n",
    "            span.end = span.start + len(new_text)\n",
    "            span_sub_start_counter = span.end - prev_end\n",
    "\n",
    "        span.text = new_text\n",
    "        \n",
    "        if should_add_span:\n",
    "            new_example_spans.append(span)\n",
    "        \n",
    "    example.text = new_example_text\n",
    "    example.spans = new_example_spans\n",
    "\n",
    "    return example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "spacy_pre = SpacyPreProcessor(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'doc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-d9bc9ab7a19b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpos_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'doc' is not defined"
     ]
    }
   ],
   "source": [
    "[t.pos_ for t in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def get_synonym(word, pos=None):\n",
    "    \"\"\"Get synonym for word given its part-of-speech (pos).\"\"\"\n",
    "    synsets = wn.synsets(word, pos=pos)\n",
    "    # Return None if wordnet has no synsets (synonym sets) for this word and pos.\n",
    "    if synsets:\n",
    "        print(synsets[0])\n",
    "        words = [lemma.name() for lemma in synsets[0].lemmas()]\n",
    "        print(\"SYNSET WORDS: \", words)\n",
    "        if words[0].lower() != word.lower():  # Skip if synonym is same as word.\n",
    "            # Multi word synonyms in wordnet use '_' as a separator e.g. reckon_with. Replace it with space.\n",
    "            return words[0].replace(\"_\", \" \")\n",
    "\n",
    "\n",
    "def replace_token(spacy_doc, idx, replacement):\n",
    "    \"\"\"Replace token in position idx with replacement.\"\"\"\n",
    "    return \" \".join([spacy_doc[:idx].text, replacement, spacy_doc[1 + idx :].text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "@operation(\"recon.v1.augment.replace_pos_with_synonym\", pre=[spacy_pre])\n",
    "def replace_pos_with_synonym(example: Example, pos: str, preprocessed_outputs={}, n_augs: int = 1):\n",
    "    \n",
    "    prev_example = example.copy(deep=True)\n",
    "    augmented_examples = {prev_example}\n",
    "    \n",
    "    pos_map = {\n",
    "        \"VERB\": \"v\",\n",
    "        \"NOUN\": \"n\",\n",
    "        \"ADJ\": \"a\"\n",
    "    }\n",
    "    \n",
    "    if pos not in pos_map:\n",
    "        raise ValueError(f\"Argument `pos` of {pos} not in {''.join(pos_map.keys())}\")\n",
    "    \n",
    "    for i in range(n_augs):\n",
    "        example = example.copy(deep=True)\n",
    "        doc = example.data.doc\n",
    "        span_starts = [s.start for s in example.spans]\n",
    "\n",
    "        # Get indices of verb tokens in sentence.\n",
    "        pos_idxs = [i for i, token in enumerate(doc) if token.pos_ == pos and token.idx not in span_starts]\n",
    "        if pos_idxs:\n",
    "            # Pick random verb idx to replace.\n",
    "            idx = np.random.choice(pos_idxs)\n",
    "            token = doc[idx]\n",
    "            synonym = get_synonym(token.text, pos=\"v\")\n",
    "            synonym = \"enjoy\"\n",
    "            print(synonym)\n",
    "            \n",
    "            \n",
    "            # If there's a valid verb synonym, replace it. Otherwise, return None.\n",
    "            if synonym:\n",
    "                curr_span = Span(text=token.text, start=token.idx, end=token.idx + len(token.text), label=\"\")\n",
    "                span_subs = {curr_span: synonym}\n",
    "                example = substitute_spans(example, span_subs)\n",
    "                if example not in augmented_examples:\n",
    "                    augmented_examples.add(example)\n",
    "                    \n",
    "    return list(augmented_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = [Example(text=\"I love running.\", spans=[Span(text=\"running\", start=7, end=14, label=\"ACTIVITY\")])]\n",
    "docs = spacy_pre(examples)\n",
    "\n",
    "for e, doc in zip(examples, docs):\n",
    "    e.doc = doc\n",
    "\n",
    "ds = Dataset(\"test\", data=examples)\n",
    "\n",
    "# ds.apply_(\"recon.v1.augment.replace_pos_with_synonym\", \"VERB\")\n",
    "\n",
    "# ds.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "Synset('love.v.01')\n",
      "SYNSET WORDS:  ['love']\n",
      "enjoy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Example(text='I enjoy running.', spans=[Span(text='running', start=8, end=15, label='ACTIVITY', token_start=None, token_end=None, kb_id=None)], tokens=None, meta={}, formatted=True, data=namespace(doc=I love running.))"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "replace_adjective_with_synonym(examples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(text='I enjoy running.', spans=[Span(text='running', start=8, end=15, label='ACTIVITY', token_start=None, token_end=None, kb_id=None)], tokens=None, meta={}, formatted=True, data=namespace(doc=I love running.))"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of recon.augmentation failed: Traceback (most recent call last):\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages/IPython/extensions/autoreload.py\", line 394, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/usr/lib/python3.6/imp.py\", line 315, in reload\n",
      "    return importlib.reload(module)\n",
      "  File \"/usr/lib/python3.6/importlib/__init__.py\", line 166, in reload\n",
      "    _bootstrap._exec(spec, module)\n",
      "  File \"<frozen importlib._bootstrap>\", line 618, in _exec\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 674, in exec_module\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 781, in get_code\n",
      "  File \"<frozen importlib._bootstrap_external>\", line 741, in source_to_code\n",
      "  File \"<frozen importlib._bootstrap>\", line 219, in _call_with_frames_removed\n",
      "  File \"/home/kabirkhan/Documents/recon/recon/augmentation.py\", line 218\n",
      "    n_augs=n_augs,\n",
      "         ^\n",
      "SyntaxError: invalid syntax\n",
      "]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting snorkel\n",
      "  Downloading https://files.pythonhosted.org/packages/e4/10/cb8602cb6b8502e7669822d1aadd199369aba0a48d732aba07bbefbd98fb/snorkel-0.9.5-py3-none-any.whl (141kB)\n",
      "\u001b[K    100% |████████████████████████████████| 143kB 2.5MB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: scipy<2.0.0,>=1.2.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from snorkel)\n",
      "Collecting torch<2.0.0,>=1.2.0 (from snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/38/53/914885a93a44b96c0dd1c36f36ff10afe341f091230aad68f7228d61db1e/torch-1.6.0-cp36-cp36m-manylinux1_x86_64.whl (748.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 748.8MB 2.2kB/s eta 0:00:011    94% |██████████████████████████████▍ | 710.1MB 16.6MB/s eta 0:00:03    97% |███████████████████████████████▏| 728.1MB 27.7MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.16.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from snorkel)\n",
      "Collecting tensorboard<2.0.0,>=1.14.0 (from snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K    100% |████████████████████████████████| 3.8MB 509kB/s eta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: tqdm<5.0.0,>=4.33.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from snorkel)\n",
      "Collecting pandas<0.26.0,>=0.25.0 (from snorkel)\n",
      "  Using cached https://files.pythonhosted.org/packages/52/3f/f6a428599e0d4497e1595030965b5ba455fd8ade6e977e3c819973c4b41d/pandas-0.25.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Collecting munkres>=1.0.6 (from snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/64/97/61ddc63578870e04db6eb1d3bee58ad4e727f682068a7c7405edb8b2cdeb/munkres-1.1.2-py2.py3-none-any.whl\n",
      "Collecting networkx<2.4,>=2.2 (from snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/85/08/f20aef11d4c343b557e5de6b9548761811eb16e438cee3d32b1c66c8566b/networkx-2.3.zip (1.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.8MB 1.1MB/s eta 0:00:011\n",
      "\u001b[?25hCollecting scikit-learn<0.22.0,>=0.20.2 (from snorkel)\n",
      "  Using cached https://files.pythonhosted.org/packages/a0/c5/d2238762d780dde84a20b8c761f563fe882b88c5a5fb03c056547c442a19/scikit_learn-0.21.3-cp36-cp36m-manylinux1_x86_64.whl\n",
      "Requirement already satisfied: future in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from torch<2.0.0,>=1.2.0->snorkel)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "Collecting setuptools>=41.0.0 (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/ed/16/e9f5c5b86696da09298ea10c32d68ad8ea21f888e45b11aa9e615adda6c9/setuptools-49.2.1-py3-none-any.whl (789kB)\n",
      "\u001b[K    100% |████████████████████████████████| 798kB 2.0MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "Collecting absl-py>=0.4 (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/1a/53/9243c600e047bd4c3df9e69cfabc1e8004a82cac2e0c484580a78a94ba2a/absl-py-0.9.0.tar.gz (104kB)\n",
      "\u001b[K    100% |████████████████████████████████| 112kB 4.0MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting grpcio>=1.6.3 (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/5e/29/1bd649737e427a6bb850174293b4f2b72ab80dd49462142db9b81e1e5c7b/grpcio-1.30.0.tar.gz (19.7MB)\n",
      "\u001b[K    100% |████████████████████████████████| 19.7MB 91kB/s  eta 0:00:011\n",
      "\u001b[?25hCollecting protobuf>=3.6.0 (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/63/14/dc43f81adc543c435cfeb45dd4ac048a97a1eb621c2ccb68ab3d15118737/protobuf-3.12.4-cp36-cp36m-manylinux1_x86_64.whl (1.3MB)\n",
      "\u001b[K    100% |████████████████████████████████| 1.3MB 1.4MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting werkzeug>=0.11.15 (from tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "  Downloading https://files.pythonhosted.org/packages/cc/94/5f7079a0e00bd6863ef8f1da638721e9da21e5bacee597595b318f71d62e/Werkzeug-1.0.1-py2.py3-none-any.whl (298kB)\n",
      "\u001b[K    100% |████████████████████████████████| 307kB 3.1MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting pytz>=2017.2 (from pandas<0.26.0,>=0.25.0->snorkel)\n",
      "  Using cached https://files.pythonhosted.org/packages/4f/a4/879454d49688e2fad93e59d7d4efda580b783c745fd2ec2a3adf87b0808d/pytz-2020.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from pandas<0.26.0,>=0.25.0->snorkel)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from networkx<2.4,>=2.2->snorkel)\n",
      "Requirement already satisfied: joblib>=0.11 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from scikit-learn<0.22.0,>=0.20.2->snorkel)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from markdown>=2.6.8->tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/kabirkhan/Documents/recon/.venv/lib/python3.6/site-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<2.0.0,>=1.14.0->snorkel)\n",
      "Building wheels for collected packages: networkx, absl-py, grpcio\n",
      "  Running setup.py bdist_wheel for networkx ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/kabirkhan/.cache/pip/wheels/de/63/64/3699be2a9d0ccdb37c7f16329acf3863fd76eda58c39c737af\n",
      "  Running setup.py bdist_wheel for absl-py ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/kabirkhan/.cache/pip/wheels/8e/28/49/fad4e7f0b9a1227708cbbee4487ac8558a7334849cb81c813d\n",
      "  Running setup.py bdist_wheel for grpcio ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/kabirkhan/.cache/pip/wheels/1e/e9/13/2ce6c99171a977bad2f9a4bccfa596a6e7ea060b9fbff51bc3\n",
      "Successfully built networkx absl-py grpcio\n",
      "Installing collected packages: torch, setuptools, absl-py, grpcio, protobuf, werkzeug, tensorboard, pytz, pandas, munkres, networkx, scikit-learn, snorkel\n",
      "  Found existing installation: setuptools 39.0.1\n",
      "    Uninstalling setuptools-39.0.1:\n",
      "      Successfully uninstalled setuptools-39.0.1\n",
      "Successfully installed absl-py-0.9.0 grpcio-1.30.0 munkres-1.1.2 networkx-2.3 pandas-0.25.3 protobuf-3.12.4 pytz-2020.1 scikit-learn-0.21.3 setuptools-49.2.1 snorkel-0.9.5 tensorboard-1.15.0 torch-1.6.0 werkzeug-1.0.1\n"
     ]
    }
   ],
   "source": [
    "!pip install snorkel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting names\n",
      "  Downloading https://files.pythonhosted.org/packages/44/4e/f9cb7ef2df0250f4ba3334fbdabaa94f9c88097089763d8e85ada8092f84/names-0.3.0.tar.gz (789kB)\n",
      "\u001b[K    100% |████████████████████████████████| 798kB 1.6MB/s eta 0:00:01\n",
      "\u001b[?25hBuilding wheels for collected packages: names\n",
      "  Running setup.py bdist_wheel for names ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/kabirkhan/.cache/pip/wheels/f9/a5/e1/be3e0aaa6fa285575078fa2aafd9959b45bdbc8de8a6803aeb\n",
      "Successfully built names\n",
      "Installing collected packages: names\n",
      "Successfully installed names-0.3.0\n"
     ]
    }
   ],
   "source": [
    "!pip install names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "from snorkel.preprocess.nlp import SpacyPreprocessor\n",
    "\n",
    "spacy = SpacyPreprocessor(text_field=\"text\", doc_field=\"doc\", memoize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import names\n",
    "from snorkel.augmentation import transformation_function\n",
    "\n",
    "# Pregenerate some random person names to replace existing ones with\n",
    "# for the transformation strategies below\n",
    "replacement_names = [names.get_full_name() for _ in range(50)]\n",
    "\n",
    "\n",
    "# Replace a random named entity with a different entity of the same type.\n",
    "@transformation_function(pre=[spacy])\n",
    "def change_person(x):\n",
    "    person_names = [ent.text for ent in x.doc.ents if ent.label_ == \"PERSON\"]\n",
    "    # If there is at least one person name, replace a random one. Else return None.\n",
    "    if person_names:\n",
    "        name_to_replace = np.random.choice(person_names)\n",
    "        replacement_name = np.random.choice(replacement_names)\n",
    "        x.text = x.text.replace(name_to_replace, replacement_name)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Swap two adjectives at random.\n",
    "@transformation_function(pre=[spacy])\n",
    "def swap_adjectives(x):\n",
    "    adjective_idxs = [i for i, token in enumerate(x.doc) if token.pos_ == \"ADJ\"]\n",
    "    # Check that there are at least two adjectives to swap.\n",
    "    if len(adjective_idxs) >= 2:\n",
    "        idx1, idx2 = sorted(np.random.choice(adjective_idxs, 2, replace=False))\n",
    "        # Swap tokens in positions idx1 and idx2.\n",
    "        x.text = \" \".join(\n",
    "            [\n",
    "                x.doc[:idx1].text,\n",
    "                x.doc[idx2].text,\n",
    "                x.doc[1 + idx1 : idx2].text,\n",
    "                x.doc[idx1].text,\n",
    "                x.doc[1 + idx2 :].text,\n",
    "            ]\n",
    "        )\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<recon.operations.Operation at 0x7fd2916a6a20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "@operation(\"recon.v1.augment.ent_label_sub\", handles_tokens=False)\n",
    "def ent_label_sub(\n",
    "    example: Example, label: str, subs: List[str], n_augs: int = 1, sub_prob: float = 0.5\n",
    ") -> List[Example]:\n",
    "    \"\"\"Augmentation to substitute entities based on label.\n",
    "    Applies a mask to the entities that have a provided label based on substitution_prob\n",
    "    and selects a random choice from the list of provided substitutions to replace each\n",
    "    span with\n",
    "\n",
    "    Args:\n",
    "        example (Example): Input example\n",
    "        label (str): Span label to replace\n",
    "            e.g. PERSON or LOCATION\n",
    "        subs (List[str]): List of substitutions\n",
    "            e.g. list of names if label is PERSON\n",
    "        n_augs (int, optional): Maximum number of augmentated examples to create per example.\n",
    "        sub_prob (float, optional): Probability from 0-1 inclusive of how many of the spans to replace\n",
    "\n",
    "    Returns:\n",
    "        List[Example]: List of augmented examples including the original.\n",
    "    \"\"\"\n",
    "\n",
    "    def augmentation(span: Span, subs: List[str]) -> Optional[str]:\n",
    "        subs = [s for s in subs if s != span.text]\n",
    "        sub = None\n",
    "        if len(subs) > 0:\n",
    "            sub = np.random.choice(subs)\n",
    "        return sub\n",
    "\n",
    "    return augment_example(example, augmentation, n_augs=n_augs, sub_prob=sub_prob, subs=subs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_transformation_function(\n",
    "    spans: List[Span],\n",
    "    span_f: Callable[[Span, Any], Optional[str]],\n",
    "    span_label: str = None,\n",
    "    sub_prob: float = 0.5,\n",
    "    **kwargs: Any\n",
    "):\n",
    "    if span_label:\n",
    "        spans = [s for s in spans if s.label == span_label]\n",
    "    mask = mask_1d(len(spans), prob=sub_prob)\n",
    "    spans_to_sub = list(np.asarray(spans)[mask])\n",
    "    \n",
    "    \n",
    "    span = np.random.choice(spans_to_sub)\n",
    "    res = span_f(span, **kwargs)\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ent_label_sub_augmentation(span: Span, subs: List[str]) -> Optional[str]:\n",
    "    subs = [s for s in subs if s != span.text]\n",
    "    sub = None\n",
    "    if len(subs) > 0:\n",
    "        sub = np.random.choice(subs)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Example(text='I love running.', spans=[Span(text='running', start=7, end=14, label='ACTIVITY', token_start=None, token_end=None, kb_id=None)], tokens=None, meta={}, formatted=True, doc=I love running.)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_transformation_function()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.make_transformation_function(spans:List[recon.types.Span], span_f:Callable[[recon.types.Span, Any], Union[str, NoneType]], span_label:str=None, sub_prob:float=0.5, **kwargs:Any)>"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "make_transformation_function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'augmentation' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-86e2f08b84c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m@\u001b[0m\u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0maugmentation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspan\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSpan\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msubs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msubs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msubs\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mspan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0msub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'augmentation' is not defined"
     ]
    }
   ],
   "source": [
    "def augmentation(span: Span, subs: List[str]) -> Optional[str]:\n",
    "    subs = [s for s in subs if s != span.text]\n",
    "    sub = None\n",
    "    if len(subs) > 0:\n",
    "        sub = np.random.choice(subs)\n",
    "    return sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_example(\n",
    "    example: Example,\n",
    "    augmentation_f: Callable[[Span, Any], Optional[str]],\n",
    "    spans: List[Span] = None,\n",
    "    span_label: str = None,\n",
    "    n_augs: int = 1,\n",
    "    sub_prob: float = 0.5,\n",
    "    **kwargs: Any,\n",
    ") -> Optional[Example]:\n",
    "    \n",
    "\n",
    "    if spans is None:\n",
    "        spans = example.spans\n",
    "\n",
    "    prev_example = example.copy(deep=True)\n",
    "    if span_label:\n",
    "        spans = [s for s in spans if s.label == span_label]\n",
    "    mask = mask_1d(len(spans), prob=sub_prob)\n",
    "    spans_to_sub = list(np.asarray(spans)[mask])\n",
    "\n",
    "    span_subs = {}\n",
    "    for span in spans_to_sub:\n",
    "        res = span_f(span, **kwargs)  #  type: ignore\n",
    "        if res:\n",
    "            span_subs[hash(span)] = res\n",
    "\n",
    "    if not any(span_subs.values()) or len(augmented_examples) > n_augs:\n",
    "        break\n",
    "\n",
    "    example = substitute_spans(example, span_subs)\n",
    "    if hash(example) != hash(prev_example):\n",
    "        return example\n",
    "\n",
    "    return list(augmented_examples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This function is not an operation since it does not have a name.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-c62f18f14f50>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mperson_ent_sub\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/recon/recon/dataset.py\u001b[0m in \u001b[0;36mapply_\u001b[0;34m(self, operation, initial_state, *args, **kwargs)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"name\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"This function is not an operation since it does not have a name.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPrinter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mno_print\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: This function is not an operation since it does not have a name."
     ]
    }
   ],
   "source": [
    "ds.apply_(person_ent_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfs = [change_person, swap_adjectives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from snorkel.augmentation import RandomPolicy\n",
    "\n",
    "random_policy = RandomPolicy(\n",
    "    len(tfs), sequence_length=2, n_per_original=2, keep_original=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[], [0, 0], [0, 1]]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_policy.generate_for_example()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.augmentation import substitute_spans, augment_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from snorkel.augmentation.core import BaseTFApplier\n",
    "\n",
    "\n",
    "class ReconDatasetTFApplier(BaseTFApplier):\n",
    "    \n",
    "    def __init__(self, tfs, policy, span_label: str, sub_prob: float = 0.5):\n",
    "        super().__init__(tfs, policy)\n",
    "        self.span_label = span_label\n",
    "        self.sub_prob = sub_prob\n",
    "    \n",
    "    def _apply_policy_to_data_point(self, x: DataPoint) -> DataPoints:\n",
    "        \n",
    "        x_transformed = []\n",
    "        for seq in self._policy.generate_for_example():\n",
    "            x_t = x\n",
    "            # Handle empty sequence for `keep_original`\n",
    "            transform_applied = len(seq) == 0\n",
    "            # Apply TFs\n",
    "            for tf_idx in seq:\n",
    "                \n",
    "                \n",
    "                if spans is None:\n",
    "                    spans = example.spans\n",
    "\n",
    "                prev_example = x.copy(deep=True)\n",
    "                if self.span_label:\n",
    "                    spans = [s for s in spans if s.label == self.span_label]\n",
    "                mask = mask_1d(len(spans), prob=sub_prob)\n",
    "                spans_to_sub = list(np.asarray(spans)[mask])\n",
    "\n",
    "                span_subs = {}\n",
    "                tf = self._tfs[tf_idx]\n",
    "                for span in spans_to_sub:\n",
    "                    x_t_or_none = tf(span, **kwargs)  #  type: ignore\n",
    "                    if x_t_or_none is not None:\n",
    "                        transform_applied = True\n",
    "                        span_subs[hash(span)] = res\n",
    "\n",
    "                \n",
    "                x_t_or_none = tf(x_t)\n",
    "                # Update if transformation was applied\n",
    "                if x_t_or_none is not None:\n",
    "                    transform_applied = True\n",
    "                    x_t = x_t_or_none\n",
    "            # Add example if original or transformations applied\n",
    "            if transform_applied:\n",
    "                x_transformed.append(x_t)\n",
    "        return x_transformed\n",
    "\n",
    "\n",
    "    def apply(self, ds: Dataset, progress_bar: bool = True) -> Dataset:\n",
    "        x_transformed: List[Example] = []\n",
    "            \n",
    "        for _, example in tqdm(ds.data, total=len(df), disable=(not progress_bar)):\n",
    "            \n",
    "            transformed_examples = self._apply_policy_to_data_point(x)\n",
    "            \n",
    "            x_transformed.extend(examples_transformed)\n",
    "        return pd.concat(x_transformed, axis=1).T.infer_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "applier = ReconDatasetTFApplier([])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
