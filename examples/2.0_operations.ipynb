{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed',)).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "import spacy\n",
    "import srsly\n",
    "# import recon\n",
    "from recon.corpus import Corpus\n",
    "from recon.constants import NONE\n",
    "from recon.corrections import fix_annotations\n",
    "from recon.dataset import Dataset\n",
    "from recon.loaders import read_jsonl\n",
    "from recon.types import Example, PredictionError, HardestExample, NERStats, EntityCoverageStats, EntityCoverage\n",
    "from recon.stats import (\n",
    "    get_ner_stats, get_entity_coverage, get_sorted_type_counts, counts_to_probs, entropy,\n",
    "    calculate_entity_coverage_entropy, calculate_label_balance_entropy, calculate_label_distribution_similarity,\n",
    "    detect_outliers\n",
    ")\n",
    "import recon.tokenization as tokenization\n",
    "from recon.insights import get_ents_by_label, get_label_disparities, top_prediction_errors, top_label_disparities, get_hardest_examples\n",
    "from recon.recognizer import SpacyEntityRecognizer\n",
    "from recon.operations import registry\n",
    "from recon.store import ExampleStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('fix_tokenization_and_spacing',\n",
       "              <function recon.operations.operation.__call__.<locals>.factory(data, **cfg) -> recon.types.OperationResult>),\n",
       "             ('add_tokens',\n",
       "              <function recon.operations.operation.__call__.<locals>.factory(data, **cfg) -> recon.types.OperationResult>)])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.operations.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_tokenization_and_spacing = registry.operations.get(\"fix_tokenization_and_spacing\")\n",
    "add_tokens = registry.operations.get(\"add_tokens\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus.from_disk(\"./data/skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 tokenization errors.\n",
      "Found 1 unfixable tokenization errors.\n",
      "[('We are looking for a Software Development Engineer who has solid coding skills, a strong machine learning background, and is passionate about developing new AI products.', 'Software Development Engineer'), ('We are looking for a Software Development Engineer who has solid coding skills, a strong machine learning background, and is passionate about developing new AI products.', 'coding'), ('Responsibilities As a SOFTWARE DEVELOPMENT ENGINEER II you will work / collaborate with other talented engineers to build features and technologies that will affect millions of your fellow developers in the community.', 'SOFTWARE DEVELOPMENT ENGINEER ')]\n",
      "{545202776435063599} set() 0 1 105 106\n",
      "set() set() 0 0 110 110\n",
      "set() set() 0 0 96 96\n"
     ]
    }
   ],
   "source": [
    "corpus.apply_(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set() set() 0 0 105 105\n",
      "set() set() 0 0 110 110\n",
      "set() set() 0 0 96 96\n"
     ]
    }
   ],
   "source": [
    "corpus.apply_(add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus = Corpus.from_disk(\"../../CognitiveServices/API-TextAnalytics-NER.CloudServices/data/2020-03-30\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<recon.store.ExampleStore at 0x7fe35d856f98>,\n",
       " <recon.store.ExampleStore at 0x7fe35d856f98>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "large_corpus.example_store, large_corpus._train.example_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.85 ms ± 199 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hash(corpus._train), hash(corpus._dev), hash(corpus._test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.1 ms ± 8.1 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hash(large_corpus._train), hash(large_corpus._dev), hash(large_corpus._test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "872 µs ± 69.8 µs per loop (mean ± std. dev. of 7 runs, 1000 loops each)\n",
      "59.9 ms ± 8.17 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)\n"
     ]
    }
   ],
   "source": [
    "%timeit hash(corpus._train)\n",
    "%timeit hash(large_corpus._train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(str(hash(corpus._train.data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2042131441891384299"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hash(corpus._train.data[0].spans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0dd8ebeed75a419cbb1f7e0a9b6244a46e2eebfd'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus._train.commit_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function operation.__call__.<locals>.factory at 0x7f3f1df8eb70>\n"
     ]
    }
   ],
   "source": [
    "print(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus._train.operations = []\n",
    "corpus._train.global_state = {}\n",
    "corpus._dev.operations = []\n",
    "corpus._dev.global_state = {}\n",
    "corpus._test.operations = []\n",
    "corpus._test.global_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 tokenization errors.\n",
      "Found 0 unfixable tokenization errors.\n",
      "{860580076040132479}\n",
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "corpus.apply_(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OperationState(name='fix_tokenization_and_spacing', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=1, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[]),\n",
       " OperationState(name='fix_tokenization_and_spacing', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[]),\n",
       " OperationState(name='fix_tokenization_and_spacing', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus._train.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='add_tokens' status=<OperationStatus.COMPLETED: 'COMPLETED'> ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661) examples_added=0 examples_removed=0 examples_corrected=0 annotations_added=0 annotations_removed=0 annotations_corrected=0 transformations=[]\n",
      "{'fix_tokenization_and_spacing': OperationState(name='fix_tokenization_and_spacing', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[]), 'add_tokens': OperationState(name='add_tokens', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[])}\n",
      "====================================================================================================\n",
      "name='add_tokens' status=<OperationStatus.COMPLETED: 'COMPLETED'> ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661) examples_added=0 examples_removed=0 examples_corrected=0 annotations_added=0 annotations_removed=0 annotations_corrected=0 transformations=[]\n",
      "{'fix_tokenization_and_spacing': OperationState(name='fix_tokenization_and_spacing', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[]), 'add_tokens': OperationState(name='add_tokens', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[])}\n",
      "====================================================================================================\n",
      "name='add_tokens' status=<OperationStatus.COMPLETED: 'COMPLETED'> ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661) examples_added=0 examples_removed=0 examples_corrected=0 annotations_added=0 annotations_removed=0 annotations_corrected=0 transformations=[]\n",
      "{'fix_tokenization_and_spacing': OperationState(name='fix_tokenization_and_spacing', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[]), 'add_tokens': OperationState(name='add_tokens', status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2020, 4, 7, 13, 4, 42, 400661), examples_added=0, examples_removed=0, examples_corrected=0, annotations_added=0, annotations_removed=0, annotations_corrected=0, transformations=[])}\n",
      "====================================================================================================\n"
     ]
    }
   ],
   "source": [
    "corpus.apply_(add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_disk('./fixed_data/skills', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state.json']"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.listdir('./fixed_data/skills/.recon/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
