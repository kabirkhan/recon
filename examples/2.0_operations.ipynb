{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_examples = [\n",
    "    {\"text\":\"Have you used the new version of my model?\",\"spans\":[{\"start\":36,\"end\":41,\"token_start\":8,\"token_end\":8,\"label\":\"SKILL\"}],\"_input_hash\":1798863398,\"_task_hash\":1273875979,\"tokens\":[{\"text\":\"Have\",\"start\":0,\"end\":4,\"id\":0},{\"text\":\"you\",\"start\":5,\"end\":8,\"id\":1},{\"text\":\"used\",\"start\":9,\"end\":13,\"id\":2},{\"text\":\"the\",\"start\":14,\"end\":17,\"id\":3},{\"text\":\"new\",\"start\":18,\"end\":21,\"id\":4},{\"text\":\"version\",\"start\":22,\"end\":29,\"id\":5},{\"text\":\"of\",\"start\":30,\"end\":32,\"id\":6},{\"text\":\"my\",\"start\":33,\"end\":35,\"id\":7},{\"text\":\"model\",\"start\":36,\"end\":41,\"id\":8},{\"text\":\"?\",\"start\":41,\"end\":42,\"id\":9}],\"_session_id\":None,\"_view_id\":\"ner_manual\",\"answer\":\"accept\"},\n",
    "    {\"text\":\"I'd like to work as an actor or model if possible.\",\"spans\":[{\"text\":\"actor\",\"start\":23,\"end\":28,\"token_start\":7,\"token_end\":7,\"label\":\"JOB_ROLE\"},{\"text\":\"model\",\"start\":32,\"end\":37,\"token_start\":9,\"token_end\":9,\"label\":\"JOB_ROLE\"}],\"_input_hash\":895552771,\"_task_hash\":-936257555,\"tokens\":[{\"text\":\"I\",\"start\":0,\"end\":1,\"id\":0},{\"text\":\"'d\",\"start\":1,\"end\":3,\"id\":1},{\"text\":\"like\",\"start\":4,\"end\":8,\"id\":2},{\"text\":\"to\",\"start\":9,\"end\":11,\"id\":3},{\"text\":\"work\",\"start\":12,\"end\":16,\"id\":4},{\"text\":\"as\",\"start\":17,\"end\":19,\"id\":5},{\"text\":\"an\",\"start\":20,\"end\":22,\"id\":6},{\"text\":\"actor\",\"start\":23,\"end\":28,\"id\":7},{\"text\":\"or\",\"start\":29,\"end\":31,\"id\":8},{\"text\":\"model\",\"start\":32,\"end\":37,\"id\":9},{\"text\":\"if\",\"start\":38,\"end\":40,\"id\":10},{\"text\":\"possible\",\"start\":41,\"end\":49,\"id\":11},{\"text\":\".\",\"start\":49,\"end\":50,\"id\":12}],\"_session_id\":None,\"_view_id\":\"ner_manual\",\"answer\":\"accept\"},\n",
    "    {\"text\":\"We are looking for a Software Development Engineer who has solid coding skills, a strong machine learning background, and is passionate about developing new AI products.\",\"meta\":{\"id\":\"599073\",\"category\":\"Engineering\",\"subCategory\":\"Data & Applied Sciences\"},\"_input_hash\":-275722772,\"_task_hash\":868828486,\"_session_id\":None,\"_view_id\":\"ner_manual\",\"answer\":\"accept\",\"dataset\":1,\"tokens\":[{\"text\":\"We\",\"start\":0,\"end\":2,\"id\":0},{\"text\":\"are\",\"start\":3,\"end\":6,\"id\":1},{\"text\":\"looking\",\"start\":7,\"end\":14,\"id\":2},{\"text\":\"for\",\"start\":15,\"end\":18,\"id\":3},{\"text\":\"a\",\"start\":19,\"end\":20,\"id\":4},{\"text\":\"Software\",\"start\":21,\"end\":29,\"id\":5},{\"text\":\"Development\",\"start\":30,\"end\":41,\"id\":6},{\"text\":\"Engineer\",\"start\":42,\"end\":50,\"id\":7},{\"text\":\"who\",\"start\":51,\"end\":54,\"id\":8},{\"text\":\"has\",\"start\":55,\"end\":58,\"id\":9},{\"text\":\"solid\",\"start\":59,\"end\":64,\"id\":10},{\"text\":\"coding\",\"start\":65,\"end\":71,\"id\":11},{\"text\":\"skills\",\"start\":72,\"end\":78,\"id\":12},{\"text\":\",\",\"start\":78,\"end\":79,\"id\":13},{\"text\":\"a\",\"start\":80,\"end\":81,\"id\":14},{\"text\":\"strong\",\"start\":82,\"end\":88,\"id\":15},{\"text\":\"machine\",\"start\":89,\"end\":96,\"id\":16},{\"text\":\"learning\",\"start\":97,\"end\":105,\"id\":17},{\"text\":\"background\",\"start\":106,\"end\":116,\"id\":18},{\"text\":\",\",\"start\":116,\"end\":117,\"id\":19},{\"text\":\"and\",\"start\":118,\"end\":121,\"id\":20},{\"text\":\"is\",\"start\":122,\"end\":124,\"id\":21},{\"text\":\"passionate\",\"start\":125,\"end\":135,\"id\":22},{\"text\":\"about\",\"start\":136,\"end\":141,\"id\":23},{\"text\":\"developing\",\"start\":142,\"end\":152,\"id\":24},{\"text\":\"new\",\"start\":153,\"end\":156,\"id\":25},{\"text\":\"AI\",\"start\":157,\"end\":159,\"id\":26},{\"text\":\"products\",\"start\":160,\"end\":168,\"id\":27},{\"text\":\".\",\"start\":168,\"end\":169,\"id\":28}],\"spans\":[{\"start\":21,\"end\":50,\"token_start\":5,\"token_end\":7,\"label\":\"SKILL\"},{\"start\":65,\"end\":71,\"token_start\":11,\"token_end\":11,\"label\":\"SKILL\"},{\"start\":89,\"end\":105,\"token_start\":16,\"token_end\":17,\"label\":\"SKILL\"},{\"start\":142,\"end\":152,\"token_start\":24,\"token_end\":24,\"label\":\"SKILL\"},{\"start\":157,\"end\":159,\"token_start\":26,\"token_end\":26,\"label\":\"SKILL\"}]},\n",
    "    {\"text\":\"Responsibilities As a SOFTWARE DEVELOPMENT ENGINEER II you will work / collaborate with other talented engineers to build features and technologies that will affect millions of your fellow developers in the community.\",\"meta\":{\"id\":\"598569\",\"category\":\"Engineering\",\"subCategory\":\"Software Engineering\"},\"_input_hash\":-930202924,\"_task_hash\":547318084,\"_session_id\":None,\"_view_id\":\"ner_manual\",\"answer\":\"accept\",\"dataset\":1,\"tokens\":[{\"text\":\"Responsibilities\",\"start\":0,\"end\":16,\"id\":0},{\"text\":\"As\",\"start\":17,\"end\":19,\"id\":1},{\"text\":\"a\",\"start\":20,\"end\":21,\"id\":2},{\"text\":\"SOFTWARE\",\"start\":22,\"end\":30,\"id\":3},{\"text\":\"DEVELOPMENT\",\"start\":31,\"end\":42,\"id\":4},{\"text\":\"ENGINEER\",\"start\":43,\"end\":51,\"id\":5},{\"text\":\"II\",\"start\":52,\"end\":54,\"id\":6},{\"text\":\"you\",\"start\":55,\"end\":58,\"id\":7},{\"text\":\"will\",\"start\":59,\"end\":63,\"id\":8},{\"text\":\"work\",\"start\":64,\"end\":68,\"id\":9},{\"text\":\"/\",\"start\":69,\"end\":70,\"id\":10},{\"text\":\"collaborate\",\"start\":71,\"end\":82,\"id\":11},{\"text\":\"with\",\"start\":83,\"end\":87,\"id\":12},{\"text\":\"other\",\"start\":88,\"end\":93,\"id\":13},{\"text\":\"talented\",\"start\":94,\"end\":102,\"id\":14},{\"text\":\"engineers\",\"start\":103,\"end\":112,\"id\":15},{\"text\":\"to\",\"start\":113,\"end\":115,\"id\":16},{\"text\":\"build\",\"start\":116,\"end\":121,\"id\":17},{\"text\":\"features\",\"start\":122,\"end\":130,\"id\":18},{\"text\":\"and\",\"start\":131,\"end\":134,\"id\":19},{\"text\":\"technologies\",\"start\":135,\"end\":147,\"id\":20},{\"text\":\"that\",\"start\":148,\"end\":152,\"id\":21},{\"text\":\"will\",\"start\":153,\"end\":157,\"id\":22},{\"text\":\"affect\",\"start\":158,\"end\":164,\"id\":23},{\"text\":\"millions\",\"start\":165,\"end\":173,\"id\":24},{\"text\":\"of\",\"start\":174,\"end\":176,\"id\":25},{\"text\":\"your\",\"start\":177,\"end\":181,\"id\":26},{\"text\":\"fellow\",\"start\":182,\"end\":188,\"id\":27},{\"text\":\"developers\",\"start\":189,\"end\":199,\"id\":28},{\"text\":\"in\",\"start\":200,\"end\":202,\"id\":29},{\"text\":\"the\",\"start\":203,\"end\":206,\"id\":30},{\"text\":\"community\",\"start\":207,\"end\":216,\"id\":31},{\"text\":\".\",\"start\":216,\"end\":217,\"id\":32}],\"spans\":[{\"start\":22,\"end\":51,\"token_start\":3,\"token_end\":5,\"label\":\"JOB_ROLE\"},{\"start\":71,\"end\":82,\"token_start\":11,\"token_end\":11,\"label\":\"SKILL\"},{\"start\":103,\"end\":112,\"token_start\":15,\"token_end\":15,\"label\":\"JOB_ROLE\"},{\"start\":135,\"end\":147,\"token_start\":20,\"token_end\":20,\"label\":\"SKILL\"},{\"start\":189,\"end\":199,\"token_start\":28,\"token_end\":28,\"label\":\"JOB_ROLE\"}]}\n",
    "]\n",
    "\n",
    "for e in raw_examples:\n",
    "#     del e['tokens']\n",
    "    del e['_input_hash']\n",
    "    del e['_task_hash']\n",
    "#     for s in e['spans']:\n",
    "#         del s['token_start']\n",
    "#         del s['token_end']\n",
    "        \n",
    "    del_keys = [k for k in e.keys() if k not in {'text', 'spans', 'tokens'}]\n",
    "    \n",
    "    for k in del_keys:\n",
    "        del e[k]\n",
    "\n",
    "import json\n",
    "for e in raw_examples:\n",
    "    print(json.dumps(e) + \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from pprint import pprint\n",
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Set, Tuple\n",
    "import spacy\n",
    "import srsly\n",
    "# import recon\n",
    "from recon.corpus import Corpus\n",
    "from recon.operations.corrections import fix_annotations\n",
    "from recon.dataset import Dataset\n",
    "from recon.loaders import read_jsonl\n",
    "from recon.types import Correction, Example, PredictionError, HardestExample, NERStats, EntityCoverageStats, EntityCoverage, Transformation, TransformationType, OperationState\n",
    "from recon.stats import (\n",
    "    get_ner_stats, get_entity_coverage, get_sorted_type_counts, get_probs_from_counts, entropy,\n",
    "    calculate_entity_coverage_entropy, calculate_label_balance_entropy, calculate_label_distribution_similarity,\n",
    "    detect_outliers\n",
    ")\n",
    "import recon.operations.tokenization as tokenization\n",
    "from recon.insights import get_ents_by_label, get_label_disparities, top_prediction_errors, top_label_disparities, get_hardest_examples\n",
    "from recon.recognizer import SpacyEntityRecognizer\n",
    "from recon.operations import registry\n",
    "from recon.store import ExampleStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'recon.v1.augment.ent_label_sub': <recon.operations.core.Operation at 0x117af1c00>,\n",
       " 'recon.v1.rename_labels': <recon.operations.core.Operation at 0x127942020>,\n",
       " 'recon.v1.fix_annotations': <recon.operations.core.Operation at 0x127942110>,\n",
       " 'recon.v1.strip_annotations': <recon.operations.core.Operation at 0x127942170>,\n",
       " 'recon.v1.split_sentences': <recon.operations.core.Operation at 0x1279421d0>,\n",
       " 'recon.v1.fix_tokenization_and_spacing': <recon.operations.core.Operation at 0x127942320>,\n",
       " 'recon.v1.add_tokens': <recon.operations.core.Operation at 0x127942380>,\n",
       " 'recon.v1.upcase_labels': <recon.operations.core.Operation at 0x1279425c0>,\n",
       " 'recon.v1.filter_overlaps': <recon.operations.core.Operation at 0x127942650>}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "registry.operations.get_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_tokenization_and_spacing = registry.operations.get(\"recon.v1.fix_tokenization_and_spacing\")\n",
    "add_tokens = registry.operations.get(\"recon.v1.add_tokens\")\n",
    "rename_labels = registry.operations.get(\"recon.v1.rename_labels\")\n",
    "fix_annotations = registry.operations.get(\"recon.v1.fix_annotations\")\n",
    "upcase_labels = registry.operations.get(\"recon.v1.upcase_labels\")\n",
    "filter_overlaps = registry.operations.get(\"recon.v1.filter_overlaps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = Corpus.from_disk(\"./data/skills\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus._train.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":106,\n",
      "    \"n_examples_no_entities\":29,\n",
      "    \"n_annotations\":243,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"SKILL\":197,\n",
      "        \"PRODUCT\":33,\n",
      "        \"JOB_ROLE\":10,\n",
      "        \"skill\":2,\n",
      "        \"product\":1\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(get_ner_stats(corpus._train.data, serialize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Applying operation 'recon.v1.fix_tokenization_and_spacing' to dataset 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 0/106 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ      => Running preprocessor recon.v1.spacy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                             | 0/106 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [00:00<00:00, 116.05it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.fix_tokenization_and_spacing'\u001b[0m\n",
      "=> Applying operation 'recon.v1.fix_tokenization_and_spacing' to dataset 'dev'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                             | 0/110 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ      => Running preprocessor recon.v1.spacy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                             | 0/110 [00:00<?, ?it/s]\u001b[A\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:00<00:00, 110.57it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.fix_tokenization_and_spacing'\u001b[0m\n",
      "=> Applying operation 'recon.v1.fix_tokenization_and_spacing' to dataset 'test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                                              | 0/96 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;4mℹ      => Running preprocessor recon.v1.spacy\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                                              | 0/96 [00:00<?, ?it/s]\u001b[A\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:01<00:00, 95.85it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.fix_tokenization_and_spacing'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus.apply_(\"recon.v1.fix_tokenization_and_spacing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OperationState(name='recon.v1.fix_tokenization_and_spacing', batch=False, args=(), kwargs={}, status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2022, 4, 10, 19, 47, 25, 440013), examples_added=0, examples_removed=0, examples_changed=0, transformations=[])]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus._train.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Applying operation 'recon.v1.upcase_labels' to dataset 'train'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 106/106 [00:01<00:00, 95.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.upcase_labels'\u001b[0m\n",
      "=> Applying operation 'recon.v1.upcase_labels' to dataset 'dev'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 110/110 [00:01<00:00, 95.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.upcase_labels'\u001b[0m\n",
      "=> Applying operation 'recon.v1.upcase_labels' to dataset 'test'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 96/96 [00:00<00:00, 99.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Completed operation 'recon.v1.upcase_labels'\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corpus.apply_(\"recon.v1.upcase_labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"n_examples\":106,\n",
      "    \"n_examples_no_entities\":29,\n",
      "    \"n_annotations\":243,\n",
      "    \"n_annotations_per_type\":{\n",
      "        \"SKILL\":199,\n",
      "        \"PRODUCT\":34,\n",
      "        \"JOB_ROLE\":10\n",
      "    },\n",
      "    \"examples_with_type\":null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(corpus.apply(get_ner_stats, serialize=True).train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[OperationState(name='recon.v1.fix_tokenization_and_spacing', batch=False, args=(), kwargs={}, status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2022, 4, 10, 19, 47, 25, 440013), examples_added=0, examples_removed=0, examples_changed=0, transformations=[]),\n",
       " OperationState(name='recon.v1.upcase_labels', batch=False, args=(), kwargs={}, status=<OperationStatus.COMPLETED: 'COMPLETED'>, ts=datetime.datetime(2022, 4, 10, 19, 47, 25, 440013), examples_added=0, examples_removed=0, examples_changed=3, transformations=[Transformation(prev_example=183104467179452678, example=594969270528231551, type=<TransformationType.EXAMPLE_CHANGED: 'EXAMPLE_CHANGED'>), Transformation(prev_example=3933994642316611414, example=4880013973796351025, type=<TransformationType.EXAMPLE_CHANGED: 'EXAMPLE_CHANGED'>), Transformation(prev_example=1365997553723636798, example=6743399603562727171, type=<TransformationType.EXAMPLE_CHANGED: 'EXAMPLE_CHANGED'>)])]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus._train.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_disk('./fixed_data/skills', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2 = Corpus.from_disk('./fixed_data/skills')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 5, 5)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpus._train.operations), len(corpus._dev.operations), len(corpus._test.operations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': {'model'}, 'dev': set(), 'test': set(), 'all': {'model'}}"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus2.apply(get_label_disparities, \"SKILL\", \"JOB_ROLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3.  4.  5.]\n",
      " [ 6.  7.  8.  9. 10.]\n",
      " [11. 12. 13. 14. 15.]\n",
      " [16. 17. 18. 19. 20.]\n",
      " [21. 22. 23. 24. 25.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ True, False,  True, False,  True],\n",
       "       [False,  True, False,  True, False],\n",
       "       [ True, False,  True, False,  True],\n",
       "       [False,  True, False,  True, False],\n",
       "       [ True, False,  True, False,  True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    " \n",
    "# Create a 5 x 5 ndarray with consecutive integers from 1 to 25 (inclusive).\n",
    "# Afterwards use Boolean indexing to pick out only the odd numbers in the array\n",
    " \n",
    "# Create a 5 x 5 ndarray with consecutive integers from 1 to 25 (inclusive).\n",
    "X = np.linspace(1,25,25).reshape(5,5)\n",
    "print(X)\n",
    " \n",
    "# Use Boolean indexing to pick out only the odd numbers in the array\n",
    "X * X % 2 == 1\n",
    "# Y = X[]\n",
    "# print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Correction(annotation='model', from_label='PRODUCT', to_label='SKILL')]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def corrections_from_dict(corrections_dict: Dict[str, Any]):\n",
    "    corrections: List[Correction] = []\n",
    "    for key, val in corrections_dict.items():\n",
    "        if isinstance(val, str):\n",
    "            from_label = \"ANY\"\n",
    "            to_label = val\n",
    "        elif isinstance(val, tuple):\n",
    "            from_label = val[0]\n",
    "            to_label = val[1]\n",
    "        else:\n",
    "            raise ValueError(\"Cannot parse corrections dict. Value must be either a str of the label \" +\n",
    "                             \"to change the annotation to (TO_LABEL) or a tuple of (FROM_LABEL, TO_LABEL)\")\n",
    "        corrections.append(Correction(\n",
    "            annotation=key,\n",
    "            from_label=from_label,\n",
    "            to_label=to_label\n",
    "        ))\n",
    "    return corrections\n",
    "\n",
    "corrections_from_dict({\n",
    "    \"model\": (\"PRODUCT\", \"SKILL\"),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'corpus2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-27fb01a68940>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcorpus2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfix_annotations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"SKILL\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'corpus2' is not defined"
     ]
    }
   ],
   "source": [
    "corrections = [\n",
    "    Correction(text=\"\")\n",
    "]\n",
    "corpus2.apply_(fix_annotations, {\"model\": \"SKILL\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['fix_tokenization_and_spacing', 'add_tokens', 'fix_annotations']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[op.name for op in corpus2._train.operations]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus2.to_disk('./fixed_data/skills_2', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus.apply_(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus.apply_(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus.apply_(add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = tokenization.fix_tokenization_and_spacing(prev_data)\n",
    "\n",
    "from timeit import default_timer as timer\n",
    "print(\"RUNNING OPERATION\\n\")\n",
    "\n",
    "start = timer()\n",
    "\n",
    "initial_len = len(prev_data)\n",
    "new_data_len = len(new_data)\n",
    "\n",
    "def get_examples_and_texts(data: List[Example]) -> Tuple[Dict[int, int], Dict[str, int]]:\n",
    "    examples: Dict[int, int] = {}\n",
    "    texts: Dict[str, int] = {}\n",
    "\n",
    "    for i, e in enumerate(data):\n",
    "        examples[hash(e)] = i\n",
    "        texts[e.text_hash()] = i\n",
    "\n",
    "    return examples, texts\n",
    "\n",
    "prev_example_hashes, prev_texts = get_examples_and_texts(prev_data)\n",
    "new_example_hashes, new_texts = get_examples_and_texts(new_data)\n",
    "\n",
    "prev_diff = set(prev_example_hashes).difference(set(new_example_hashes))\n",
    "next_diff = set(new_example_hashes).difference(set(prev_example_hashes))\n",
    "\n",
    "prev_examples_to_hashes = {prev_example_hashes[pd]: pd for pd in prev_diff}\n",
    "next_examples_to_hashes = {new_example_hashes[nd]: nd for nd in next_diff}\n",
    "\n",
    "examples_added = max(len(next_diff) - len(prev_diff), 0)\n",
    "examples_removed = max(len(prev_diff) - len(next_diff), 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "seen = set()\n",
    "transformations = set()\n",
    "\n",
    "for pd in prev_diff:\n",
    "    if pd in new_example_hashes:\n",
    "        prev_example_index = prev_example_hashes[pd]\n",
    "        new_example_hashes[pd] = prev_example_index\n",
    "    prev_example_index = prev_example_hashes[pd]\n",
    "    if prev_example_index in next_examples_to_hashes:\n",
    "        new_example_hash = next_examples_to_hashes[prev_example_index]\n",
    "        transformation = Transformation(prev_example=pd, example=new_example_hash, type=TransformationType.EXAMPLE_CHANGED)\n",
    "    else:\n",
    "        transformation = Transformation(prev_example=pd, example=-1, type=TransformationType.EXAMPLE_REMOVED)\n",
    "    \n",
    "    if pd not in seen and new_example_hash not in seen:\n",
    "        transformations.add(transformation)\n",
    "        seen.add(pd)\n",
    "        seen.add(new_example_hash)\n",
    "\n",
    "# seen = set()\n",
    "\n",
    "for nd in next_diff:\n",
    "    next_example_index = new_example_hashes[nd]\n",
    "    if next_example_index in prev_examples_to_hashes:\n",
    "        prev_example_hash = prev_examples_to_hashes[next_example_index]\n",
    "        transformation = Transformation(prev_example=prev_example_hash, example=nd, type=TransformationType.EXAMPLE_CHANGED)\n",
    "    else:\n",
    "        transformation = Transformation(prev_example=-1, example=nd, type=TransformationType.EXAMPLE_ADDED)\n",
    "        \n",
    "    if nd not in seen and prev_example_hash not in seen:\n",
    "        transformations.add(transformation)\n",
    "        seen.add(nd)\n",
    "        seen.add(prev_example_hash)\n",
    "\n",
    "        \n",
    "print(prev_diff)\n",
    "print(next_diff)\n",
    "print(\"Examples Added: \", examples_added)\n",
    "print(\"Examples Removed: \", examples_removed)\n",
    "print(\"Transformations: \", len(transformations))\n",
    "print(\"Transformations (ADDED): \", len([t for t in transformations if t.type == TransformationType.EXAMPLE_ADDED]))\n",
    "print(\"Transformations (REMOVED): \", len([t for t in transformations if t.type == TransformationType.EXAMPLE_REMOVED]))\n",
    "print(\"Transformations (CHANGED): \", len([t for t in transformations if t.type == TransformationType.EXAMPLE_CHANGED]))\n",
    "\n",
    "\n",
    "\n",
    "mutual_diff = set(prev_example_hashes) ^ set(new_example_hashes)\n",
    "end = timer()\n",
    "print(\"Total operation calculation time: \", round(end - start, 2))\n",
    "\n",
    "\n",
    "# from dictdiffer import diff\n",
    "# start = timer()\n",
    "# hash_diff = list(diff(prev_data, new_data))\n",
    "# end = timer()\n",
    "\n",
    "# print(\"Total diff of examples: \", round(end - start, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(hash_diff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(hash_diff[0][2][0].json())\n",
    "hash_diff[0][2][1].json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([d for d in hash_diff if d[0] == \"remove\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "large_corpus.example_store, large_corpus._train.example_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hash(corpus._train), hash(corpus._dev), hash(corpus._test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hash(large_corpus._train), hash(large_corpus._dev), hash(large_corpus._test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%timeit hash(corpus._train)\n",
    "%timeit hash(large_corpus._train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(str(hash(corpus._train.data[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hash(corpus._train.data[0].spans[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus._train.commit_hash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus._train.operations = []\n",
    "corpus._train.global_state = {}\n",
    "corpus._dev.operations = []\n",
    "corpus._dev.global_state = {}\n",
    "corpus._test.operations = []\n",
    "corpus._test.global_state = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.apply_(fix_tokenization_and_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus._train.operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.apply_(add_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.to_disk('./fixed_data/skills', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.listdir('./fixed_data/skills/.recon/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from recon.operations import op_iter\n",
    "from recon.preprocess import SpacyPreProcessor\n",
    "from recon.registry import tokenizers\n",
    "tokenizer = tokenizers.get(\"default\")\n",
    "nlp = tokenizer()\n",
    "spacy_preprocessor = SpacyPreProcessor(nlp)\n",
    "\n",
    "\n",
    "def test_add_tokens():\n",
    "    # fmt: off\n",
    "    untokenized_examples = [Example(**example) for example in [\n",
    "        {\"text\": \"Have you used the new version of my model?\", \"spans\": [{\"start\": 36, \"end\": 41, \"label\": \"SKILL\"}]},\n",
    "        {\"text\": \"I'd like to work as an actor or model if possible.\", \"spans\": [{\"text\": \"actor\", \"start\": 23, \"end\": 28, \"label\": \"JOB_ROLE\"}, {\"text\": \"model\", \"start\": 32, \"end\": 37, \"label\": \"JOB_ROLE\"}]},\n",
    "        {\"text\": \"We are looking for a Software Development Engineer who has solid coding skills, a strong machine learning background, and is passionate about developing new AI products.\", \"spans\": [{\"start\": 21, \"end\": 50, \"label\": \"SKILL\"}, {\"start\": 65, \"end\": 71, \"label\": \"SKILL\"}, {\"start\": 89, \"end\": 105, \"label\": \"SKILL\"}, {\"start\": 142, \"end\": 152, \"label\": \"SKILL\"}, {\"start\": 157, \"end\": 159, \"label\": \"SKILL\"}]},\n",
    "        {\"text\": \"Responsibilities As a SOFTWARE DEVELOPMENT ENGINEER II you will work / collaborate with other talented engineers to build features and technologies that will affect millions of your fellow developers in the community.\", \"spans\": [{\"start\": 22, \"end\": 51, \"label\": \"JOB_ROLE\"}, {\"start\": 71, \"end\": 82, \"label\": \"SKILL\"}, {\"start\": 103, \"end\": 112, \"label\": \"JOB_ROLE\"}, {\"start\": 135, \"end\": 147, \"label\": \"SKILL\"}, {\"start\": 189, \"end\": 199, \"label\": \"JOB_ROLE\"}]}\n",
    "    ]]\n",
    "\n",
    "    tokenized_examples = [Example(**example) for example in [\n",
    "        {\"text\": \"Have you used the new version of my model?\", \"spans\": [{\"start\": 36, \"end\": 41, \"token_start\": 8, \"token_end\": 8, \"label\": \"SKILL\"}], \"tokens\": [{\"text\": \"Have\", \"start\": 0, \"end\": 4, \"id\": 0}, {\"text\": \"you\", \"start\": 5, \"end\": 8, \"id\": 1}, {\"text\": \"used\", \"start\": 9, \"end\": 13, \"id\": 2}, {\"text\": \"the\", \"start\": 14, \"end\": 17, \"id\": 3}, {\"text\": \"new\", \"start\": 18, \"end\": 21, \"id\": 4}, {\"text\": \"version\", \"start\": 22, \"end\": 29, \"id\": 5}, {\"text\": \"of\", \"start\": 30, \"end\": 32, \"id\": 6}, {\"text\": \"my\", \"start\": 33, \"end\": 35, \"id\": 7}, {\"text\": \"model\", \"start\": 36, \"end\": 41, \"id\": 8}, {\"text\": \"?\", \"start\": 41, \"end\": 42, \"id\": 9}]},\n",
    "        {\"text\": \"I'd like to work as an actor or model if possible.\", \"spans\": [{\"text\": \"actor\", \"start\": 23, \"end\": 28, \"token_start\": 7, \"token_end\": 7, \"label\": \"JOB_ROLE\"}, {\"text\": \"model\", \"start\": 32, \"end\": 37, \"token_start\": 9, \"token_end\": 9, \"label\": \"JOB_ROLE\"}], \"tokens\": [{\"text\": \"I\", \"start\": 0, \"end\": 1, \"id\": 0}, {\"text\": \"'d\", \"start\": 1, \"end\": 3, \"id\": 1}, {\"text\": \"like\", \"start\": 4, \"end\": 8, \"id\": 2}, {\"text\": \"to\", \"start\": 9, \"end\": 11, \"id\": 3}, {\"text\": \"work\", \"start\": 12, \"end\": 16, \"id\": 4}, {\"text\": \"as\", \"start\": 17, \"end\": 19, \"id\": 5}, {\"text\": \"an\", \"start\": 20, \"end\": 22, \"id\": 6}, {\"text\": \"actor\", \"start\": 23, \"end\": 28, \"id\": 7}, {\"text\": \"or\", \"start\": 29, \"end\": 31, \"id\": 8}, {\"text\": \"model\", \"start\": 32, \"end\": 37, \"id\": 9}, {\"text\": \"if\", \"start\": 38, \"end\": 40, \"id\": 10}, {\"text\": \"possible\", \"start\": 41, \"end\": 49, \"id\": 11}, {\"text\": \".\", \"start\": 49, \"end\": 50, \"id\": 12}]},\n",
    "        {\"text\": \"We are looking for a Software Development Engineer who has solid coding skills, a strong machine learning background, and is passionate about developing new AI products.\", \"tokens\": [{\"text\": \"We\", \"start\": 0, \"end\": 2, \"id\": 0}, {\"text\": \"are\", \"start\": 3, \"end\": 6, \"id\": 1}, {\"text\": \"looking\", \"start\": 7, \"end\": 14, \"id\": 2}, {\"text\": \"for\", \"start\": 15, \"end\": 18, \"id\": 3}, {\"text\": \"a\", \"start\": 19, \"end\": 20, \"id\": 4}, {\"text\": \"Software\", \"start\": 21, \"end\": 29, \"id\": 5}, {\"text\": \"Development\", \"start\": 30, \"end\": 41, \"id\": 6}, {\"text\": \"Engineer\", \"start\": 42, \"end\": 50, \"id\": 7}, {\"text\": \"who\", \"start\": 51, \"end\": 54, \"id\": 8}, {\"text\": \"has\", \"start\": 55, \"end\": 58, \"id\": 9}, {\"text\": \"solid\", \"start\": 59, \"end\": 64, \"id\": 10}, {\"text\": \"coding\", \"start\": 65, \"end\": 71, \"id\": 11}, {\"text\": \"skills\", \"start\": 72, \"end\": 78, \"id\": 12}, {\"text\": \",\", \"start\": 78, \"end\": 79, \"id\": 13}, {\"text\": \"a\", \"start\": 80, \"end\": 81, \"id\": 14}, {\"text\": \"strong\", \"start\": 82, \"end\": 88, \"id\": 15}, {\"text\": \"machine\", \"start\": 89, \"end\": 96, \"id\": 16}, {\"text\": \"learning\", \"start\": 97, \"end\": 105, \"id\": 17}, {\"text\": \"background\", \"start\": 106, \"end\": 116, \"id\": 18}, {\"text\": \",\", \"start\": 116, \"end\": 117, \"id\": 19}, {\"text\": \"and\", \"start\": 118, \"end\": 121, \"id\": 20}, {\"text\": \"is\", \"start\": 122, \"end\": 124, \"id\": 21}, {\"text\": \"passionate\", \"start\": 125, \"end\": 135, \"id\": 22}, {\"text\": \"about\", \"start\": 136, \"end\": 141, \"id\": 23}, {\"text\": \"developing\", \"start\": 142, \"end\": 152, \"id\": 24}, {\"text\": \"new\", \"start\": 153, \"end\": 156, \"id\": 25}, {\"text\": \"AI\", \"start\": 157, \"end\": 159, \"id\": 26}, {\"text\": \"products\", \"start\": 160, \"end\": 168, \"id\": 27}, {\"text\": \".\", \"start\": 168, \"end\": 169, \"id\": 28}], \"spans\": [{\"start\": 21, \"end\": 50, \"token_start\": 5, \"token_end\": 7, \"label\": \"SKILL\"}, {\"start\": 65, \"end\": 71, \"token_start\": 11, \"token_end\": 11, \"label\": \"SKILL\"}, {\"start\": 89, \"end\": 105, \"token_start\": 16, \"token_end\": 17, \"label\": \"SKILL\"}, {\"start\": 142, \"end\": 152, \"token_start\": 24, \"token_end\": 24, \"label\": \"SKILL\"}, {\"start\": 157, \"end\": 159, \"token_start\": 26, \"token_end\": 26, \"label\": \"SKILL\"}]},\n",
    "        {\"text\": \"Responsibilities As a SOFTWARE DEVELOPMENT ENGINEER II you will work / collaborate with other talented engineers to build features and technologies that will affect millions of your fellow developers in the community.\", \"tokens\": [{\"text\": \"Responsibilities\", \"start\": 0, \"end\": 16, \"id\": 0}, {\"text\": \"As\", \"start\": 17, \"end\": 19, \"id\": 1}, {\"text\": \"a\", \"start\": 20, \"end\": 21, \"id\": 2}, {\"text\": \"SOFTWARE\", \"start\": 22, \"end\": 30, \"id\": 3}, {\"text\": \"DEVELOPMENT\", \"start\": 31, \"end\": 42, \"id\": 4}, {\"text\": \"ENGINEER\", \"start\": 43, \"end\": 51, \"id\": 5}, {\"text\": \"II\", \"start\": 52, \"end\": 54, \"id\": 6}, {\"text\": \"you\", \"start\": 55, \"end\": 58, \"id\": 7}, {\"text\": \"will\", \"start\": 59, \"end\": 63, \"id\": 8}, {\"text\": \"work\", \"start\": 64, \"end\": 68, \"id\": 9}, {\"text\": \"/\", \"start\": 69, \"end\": 70, \"id\": 10}, {\"text\": \"collaborate\", \"start\": 71, \"end\": 82, \"id\": 11}, {\"text\": \"with\", \"start\": 83, \"end\": 87, \"id\": 12}, {\"text\": \"other\", \"start\": 88, \"end\": 93, \"id\": 13}, {\"text\": \"talented\", \"start\": 94, \"end\": 102, \"id\": 14}, {\"text\": \"engineers\", \"start\": 103, \"end\": 112, \"id\": 15}, {\"text\": \"to\", \"start\": 113, \"end\": 115, \"id\": 16}, {\"text\": \"build\", \"start\": 116, \"end\": 121, \"id\": 17}, {\"text\": \"features\", \"start\": 122, \"end\": 130, \"id\": 18}, {\"text\": \"and\", \"start\": 131, \"end\": 134, \"id\": 19}, {\"text\": \"technologies\", \"start\": 135, \"end\": 147, \"id\": 20}, {\"text\": \"that\", \"start\": 148, \"end\": 152, \"id\": 21}, {\"text\": \"will\", \"start\": 153, \"end\": 157, \"id\": 22}, {\"text\": \"affect\", \"start\": 158, \"end\": 164, \"id\": 23}, {\"text\": \"millions\", \"start\": 165, \"end\": 173, \"id\": 24}, {\"text\": \"of\", \"start\": 174, \"end\": 176, \"id\": 25}, {\"text\": \"your\", \"start\": 177, \"end\": 181, \"id\": 26}, {\"text\": \"fellow\", \"start\": 182, \"end\": 188, \"id\": 27}, {\"text\": \"developers\", \"start\": 189, \"end\": 199, \"id\": 28}, {\"text\": \"in\", \"start\": 200, \"end\": 202, \"id\": 29}, {\"text\": \"the\", \"start\": 203, \"end\": 206, \"id\": 30}, {\"text\": \"community\", \"start\": 207, \"end\": 216, \"id\": 31}, {\"text\": \".\", \"start\": 216, \"end\": 217, \"id\": 32}], \"spans\": [{\"start\": 22, \"end\": 51, \"token_start\": 3, \"token_end\": 5, \"label\": \"JOB_ROLE\"}, {\"start\": 71, \"end\": 82, \"token_start\": 11, \"token_end\": 11, \"label\": \"SKILL\"}, {\"start\": 103, \"end\": 112, \"token_start\": 15, \"token_end\": 15, \"label\": \"JOB_ROLE\"}, {\"start\": 135, \"end\": 147, \"token_start\": 20, \"token_end\": 20, \"label\": \"SKILL\"}, {\"start\": 189, \"end\": 199, \"token_start\": 28, \"token_end\": 28, \"label\": \"JOB_ROLE\"}]}\n",
    "    ]]\n",
    "    # fmt: on\n",
    "    \n",
    "    fixed_examples = []\n",
    "    for orig_example_hash, example, preprocessed_outputs in op_iter(untokenized_examples, pre=[spacy_preprocessor]):\n",
    "#         print(preprocessed_outputs)\n",
    "        fixed_examples.append(tokenization.add_tokens(example, preprocessed_outputs=preprocessed_outputs))\n",
    "\n",
    "    for fixed_example, tokenized_example in zip(fixed_examples, tokenized_examples):\n",
    "        assert fixed_example.text == tokenized_example.text\n",
    "        assert fixed_example.spans == tokenized_example.spans\n",
    "        assert fixed_example.tokens == tokenized_example.tokens\n",
    "        \n",
    "test_add_tokens()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
