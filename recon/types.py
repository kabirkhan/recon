from typing import Any, Dict, List, Optional

from pydantic import BaseModel, Field, Schema, validator


class Span(BaseModel):
    """Entity Label in Example"""
    text: str
    """Span text"""
    start: int
    """Span start character index in Example text."""
    end: int
    """Span end character index in Example text."""
    label: str
    """Entity label"""


class Example(BaseModel):
    """Example with NER Label spans
    
    Attributes:
        text (str): Example text
        spans (List[Span]): List of entity spans
        meta (Dict[str, Any], optional): Meta information about the example
    """
    text: str
    spans: List[Span]
    meta: Dict[str, Any] = {}


class PredictionErrorExamplePair(BaseModel):
    """Dataclass representation of original Example in a PredictionError
    vs the predicted Example from an EntityRecognizer 
    
    Attributes:
        original (Example): Original Example
        spans (List[Span]): List of entity spans
        meta (Dict[str, Any], optional): Meta information about the example
    """
    original: Example
    predicted: Example


class PredictionError(BaseModel):
    """Representation of errors an EntityRecognizer makes on a labeled dataset.
    
    Attributes:
        text (str): Span text with error
        true_label (str): True label in annotated Example
        pred_label (str): The label predicted by the EntityRecognizer
        count (int): Number of times this PredictionError occurs
        examples (List[PredictionErrorExamplePair], optional):
            List of PredictionErrorExamplePairs that have this PredictionError
    """
    text: str
    true_label: str
    pred_label: str
    count: int
    examples: Optional[List[PredictionErrorExamplePair]] = []


class HardestExample(BaseModel):
    """Container for how hard an Example is for an EntityRecognizer to predict
    all entities correctly for.
    """
    example: Example
    count: int
    prediction_errors: Optional[List[PredictionError]]


class LabelDisparity(BaseModel):
    """Container for the number of disparities in a Dataset
    where some text is tagged as label1 in some places and label2 in others
    
    Attributes:
        label1 (str): Label1
        label2 (str): Label2
        count (int): Number of times this label disparity occurs
        examples (List[Example], optional): List of Examples where this disparity occurs
    """
    label1: str
    label2: str
    count: int
    examples: Optional[List[Example]] = []


class NERStats(BaseModel):
    """Container for tracking basic NER statistics"""
    n_examples: int
    n_examples_no_entities: int
    n_annotations: int
    n_annotations_per_type: Dict[str, int]
    examples_with_type: Optional[Dict[str, Example]]


class EntityCoverage(BaseModel):
    """Container for tracking how well an Entity is covered.
    
    Attributes:
        text (str): The entity text
        label (str): The entity label
        count (int): Number of times this text/label combination occurs
        examples (List[Example], optional): List of Examples where this entity occurs
    """
    text: str
    label: str
    count: int
    examples: Optional[List[Example]] = []


class EntityCoverageStats(BaseModel):
    """Container for output of how similar the Entity Coverage of 2 datasets is"""
    entity: float
    count: float


class Outliers(BaseModel):
    """Container for low and high indices of outlier detection"""
    low: List[int]
    high: List[int]